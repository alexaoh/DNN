---
title: "Task 2"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: hide
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 1234
  DA.valid: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", warning = F)
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

library(png)
#library(imager)
library(tfdatasets)

set.seed(params$seed)
```

The objective of this task is diagnosis of chest X-ray images. More specifically, we want to be able to separate between normal X-ray images and X-ray images with effusion. 

We are given two data sets of images; 500 normal X-ray images and 500 X-ray images with effusion. The images are selected from the public [NIH ChestXray14](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community) data set. This data will be used to construct models for diagnosis of future (unseen) images. 

# Preprocessing

First we separate the data into two sets; a training set containing $\frac23$ of the data and a test set containing the remaining $\frac13$. In order to do this, we assume that both sets of images are saved in a directory called "images". This directory contains the 500 normal X-ray images in a sub directory called "normal", and the 500 X-ray images with effusion in a sub directory called "effusion". The images are given in png format. After this split, we split the training data into a "pure" training set and a validation data set, where the former takes $\frac45$ of the values and the latter takes $\frac15$ of the values (thus, in total data set terms, the former takes $\frac{8}{15}$ and the latter takes $\frac{2}{15}$ of the complete data set. The remaining $\frac13$ is still left to the test set). 

We create three (or two, without the validation directory, depending on the parameter `DA.valid`) new sub directories within the "images" directory; one called "test", which contains the randomly sampled test data, one called "train", which contains the randomly sampled training data and one called "valid", which contains the randomly sample validation data, which is sampled from the training data. The last directory is only created if `params$DA.valid` is set to `TRUE`. The reason behind this parameter will be explained later. We assume that these directories have not been created on the computer earlier (this is very important in order for the data to be divided correctly between the directories!). We could implement some sort of deletion of old directories if they already exist, but I have chosen to skip this for now. 

```{r}
set.seed(params$seed) # Set seed to 1234, as chosen in params. 
image.path <- "./images"
normal.path <- paste0(image.path, "/normal")
effusion.path <- paste0(image.path, "/effusion") 

# New path for training and test images. 
train.path <- paste0(image.path, "/train") # New path for training images. 
test.path <- paste0(image.path, "/test") # New path for test images. 

if(!params$DA.valid){
  valid.path <- paste0(image.path, "/valid") # New path for validation images. 
}

# Sub directories of the above paths, for training and test images, split up into "normal" and "effusion":
train.normal.path <- paste0(train.path, "/normal") # New path for normal train images . 
train.effusion.path <- paste0(train.path, "/effusion") # New path for effusion train images.
test.normal.path <- paste0(test.path, "/normal") # New path for normal test images.  
test.effusion.path <- paste0(test.path, "/effusion") # New path for effusion test images. 

if(!params$DA.valid){
  valid.normal.path <- paste0(valid.path, "/normal") # New path for normal validation images. 
  valid.effusion.path <- paste0(valid.path, "/effusion") # New path for effusion validation images.  
}


# We collect the names of all the images.
normal.image.names <- list.files(normal.path)
effusion.image.names <- list.files(effusion.path)

# We create the two new directories. If they already exist, a warning is printed. 
dir.create(train.path)
dir.create(test.path)

if(!params$DA.valid){
  dir.create(valid.path)
}

# We also create "effusion" and "normal" sub directories inside the new "train" and "test" directories. 
# This is needed to get images for models later. 
dir.create(train.normal.path)
dir.create(test.normal.path)
dir.create(train.effusion.path)
dir.create(test.effusion.path)

if(!params$DA.valid){
  dir.create(valid.normal.path)
  dir.create(valid.effusion.path)
}

# We fill the newly created sub folders with randomly sampled data. 
# We know that the images are named (for example) "normal0.png" and "effusion0.png", 
# with numbers ranging from 0 to 499. We confirm that this is the case below. 
all(sort(parse_number(normal.image.names)) == 0:499)
all(sort(parse_number(effusion.image.names)) == 0:499)

# We sample from 0:499, in order to choose training numbers.
train.numbers <- sample(0:499, size = 2/3*500)
# Find testing numbers as well, simply using a set difference. 
test.numbers <- setdiff(0:499, train.numbers)

if(!params$DA.valid){
  validation.numbers <- sample(train.numbers, size = 1/5*(2/3*500))
  train.numbers <- setdiff(train.numbers, validation.numbers)
  test.numbers <- setdiff(0:499, train.numbers)
  test.numbers <- setdiff(test.numbers, validation.numbers)
}

# Next we select the image names corresponding to the indices sampled above. 
normal.image.names <- data.frame(normal.image.names)
effusion.image.names <- data.frame(effusion.image.names)

# Select image names corresponding to training numbers sampled above. 
train.normal <- normal.image.names %>% dplyr::filter(parse_number(normal.image.names) %in% train.numbers)
train.normal <- train.normal$normal.image.names # Change data type of names to vector. 
all(sort(parse_number(train.normal)) == sort(train.numbers)) # Check that we have selected the correct names.

test.normal <- normal.image.names %>% dplyr::filter(parse_number(normal.image.names) %in% test.numbers)
test.normal <- test.normal$normal.image.names # Change data type of names to vector. 
all(sort(parse_number(test.normal)) == sort(test.numbers)) # Check that we have selected the correct names.

if(!params$DA.valid){
  validation.normal <- normal.image.names %>% dplyr::filter(parse_number(normal.image.names) %in% validation.numbers)
  validation.normal <- validation.normal$normal.image.names # Change data type of names to vector. 
  print(all(sort(parse_number(validation.normal)) == sort(validation.numbers))) # Check that we have selected the correct names.
}

# We sample from 0:499, in order to choose training numbers. We do it again for the effusion images, 
# in case the indices of the two types of images are not chosen randomly, 
# and they present some sort of dependence (we do not know how the images are named by the source).
train.numbers <- sample(0:499, size = 2/3*500)
# Find testing numbers as well, simply using a set difference. 
test.numbers <- setdiff(0:499, train.numbers)

if(!params$DA.valid){
  validation.numbers <- sample(train.numbers, size = 1/5*(2/3*500))
  train.numbers <- setdiff(train.numbers, validation.numbers)
  test.numbers <- setdiff(0:499, train.numbers)
  test.numbers <- setdiff(test.numbers, validation.numbers)
}

# Next we select the image names corresponding to the indices sampled above. 
train.effusion <- effusion.image.names %>% dplyr::filter(parse_number(effusion.image.names) %in% train.numbers)
train.effusion <- train.effusion$effusion.image.names # Change data type of names to vector. 
all(sort(parse_number(train.effusion)) == sort(train.numbers)) # Check that we have selected the correct names.

test.effusion <- effusion.image.names %>% dplyr::filter(parse_number(effusion.image.names) %in% test.numbers)
test.effusion <- test.effusion$effusion.image.names # Change data type of names to vector. 
all(sort(parse_number(test.effusion)) == sort(test.numbers)) # Check that we have selected the correct names.

if(!params$DA.valid){
  validation.effusion <- effusion.image.names %>% dplyr::filter(parse_number(effusion.image.names) %in% validation.numbers)
  validation.effusion <- validation.effusion$effusion.image.names # Change data type of names to vector. 
  print(all(sort(parse_number(validation.effusion)) == sort(validation.numbers))) # Check that we have selected the correct names.
}

# Now that we have all the file names, we copy them into their respective directories. 
for (i in 1:length(train.numbers)){
  file.copy(from = paste0(normal.path, "/", train.normal[i]), to = train.normal.path, overwrite = T)
  file.copy(from = paste0(effusion.path, "/", train.effusion[i]), to = train.effusion.path, overwrite = T)
}

for (i in 1:length(test.numbers)){
  file.copy(from = paste0(normal.path, "/", test.normal[i] ), to = test.normal.path, overwrite = T)
  file.copy(from = paste0(effusion.path, "/", test.effusion[i]), to = test.effusion.path, overwrite = T)
}

if(!params$DA.valid){
  for (i in 1:length(validation.numbers)){
    file.copy(from = paste0(normal.path, "/", validation.normal[i]), to = valid.normal.path, overwrite = T)
    file.copy(from = paste0(effusion.path, "/", validation.effusion[i]), to = valid.effusion.path, overwrite = T)
  }
}

# Check that it worked!
train.normal.new <- list.files(train.normal.path)
length(train.normal.new)
all(train.normal.new == train.normal)
train.effusion.new <- list.files(train.effusion.path)
length(train.effusion.new)
all(train.effusion.new == train.effusion)
test.normal.new <- list.files(test.normal.path)
length(test.normal.new)
all(test.normal.new == test.normal)
test.effusion.new <- list.files(test.effusion.path)
length(test.effusion.new)
all(test.effusion.new == test.effusion)
if(!params$DA.valid){
  valid.normal.new <- list.files(valid.normal.path)
  print(length(valid.normal.new))
  print(all(valid.normal.new == validation.normal))
  valid.effusion.new <- list.files(valid.effusion.path)
  print(length(valid.effusion.new))
  print(all(valid.effusion.new == validation.effusion))
}
```

# Image Data Generators 

Simply loading all 1000 images into the memory is (in most cases) not feasible. We make image data generators in order to work with models. These are image processing helper tools from Keras, which are used to turn image files on disk into batches of pre-processed tensors. The function `image_data_generator` can be used to generate batches with real-time data augmentation. Thus, this function can be used to pre-process the images. We rescale the images in $[0,1]$ using this function. Moreover, we apply some image augmentation, like shifting, zooming and change of brightness. This is done to avoid overfitting during model training, i.e. to improve the generalization abilities of the models. The function `flow_images_from_directory` takes, among other arguments, a directory and an image generator as input, and generates batches of images from the specified directory, following the pre-processing rules given in the generator. We also choose the target size of the images, i.e. the width and height, where the original images are $512 \times 512$. As mentioned earlier, we also choose a validation split of $\frac15$, which means that one fifth of the training data will be used for model validation during fitting. 

Notice that we have tried training the model both with and without data augmentation on the validation data, following the discussion (e.g.) in [this thread](https://stackoverflow.com/questions/48029542/data-augmentation-in-test-validation-set). I wanted to test both variants, since it is not completey clear which is most coherent and I think both could make sense, depending on the philosophy one follows. It is not clear to me which is the best, as they seem to give similar results according to the performance assessment. Therefore, I have chosen to set the parameter `params$DA.valid` to `TRUE` during my discussion in this report, even though both options are aptly implemented and tested simply by changing the value of the parameter. When wanting to add image augmentation to the validation data, we set the parameter `DA.valid` to `TRUE`, which then does not make the directory for validation data in the preprocessing above. The generators below are therefore defined in slightly different ways depending on the value of the parameter, where I have tried to explain the methodologies as clearly as possible in the code below. 

```{r}
img_width <- 128 # or 256, 128, 64 or 32 (try different sizes)!
img_height <- 128
target_size <- c(img_width, img_height)
batch_size <- 32
epochs <- 30
channels <- 3 # RGB = 3 channels
image_size <- c(target_size, channels)

# optional data augmentation.
train_data_gen <- image_data_generator(
  rescale = 1/255,
  validation_split = 1/5,
  # ,rotation_range = 40, # Not relevant for us. 
  # Images will almost always be relatively straight. 
  width_shift_range = 0.2, # Shift in x direction. 
  height_shift_range = 0.2, # Shift in y direction. 
  # shear_range = 0.2, # I do not want to use shearing either, 
  # as it seems a bit irrelevant for our type of images. 
  zoom_range = 0.2,
  # horizontal_flip = TRUE, # Deemed irrelevant. 
  fill_mode = "constant", # Added constant fill-mode (cval = 0), 
  # since the rest of the fill-modes seem to distort the images 
  # in a very unnatural way. Therefore I think it is better to 
  # simply set the points outside the boundaries of the input to 0. 
  brightness_range = c(0.5, 1.5) # Play with the brightness. 
)

# We do not apply the data-augmentation (except from rescaling)
# testing data set, since it is important 
# to validate on the true images we have been given. 
test_data_gen <- image_data_generator(
  rescale = 1/255
)

# Then we load the data using the generators. 


if(!params$DA.valid){
  # DO NOT USE data augmentation on the validation data. 
  # training images.
  train.image_array_gen <- flow_images_from_directory(train.path, 
                                                    train_data_gen,
                                                    class_mode = "binary",
                                                    seed = params$seed, 
                                                    target_size = target_size,
                                                    batch_size = batch_size,
                                                    #color_mode = "grayscale",
                                                    )
  
  # If we choose to NOT use data augmentation on the validation data.
  val.image_array_gen <- flow_images_from_directory(valid.path, 
                                                   test_data_gen,
                                                   class_mode = "binary",
                                                   seed = params$seed, 
                                                   target_size = target_size,
                                                   batch_size = batch_size
                                                   #color_mode = "grayscale"
                                                    )
} else {
  # USE data augmentation on the validation data. 
  train.image_array_gen <- flow_images_from_directory(train.path, 
                                                    train_data_gen,
                                                    class_mode = "binary",
                                                    seed = params$seed, 
                                                    target_size = target_size,
                                                    batch_size = batch_size,
                                                    #color_mode = "grayscale",
                                                    subset = "training"
                                                    )
  
  # If we choose to use data augmentation on the validation data, 
  # then we use the same image data generator as for the training data. 
  val.image_array_gen <- flow_images_from_directory(train.path, 
                                                   train_data_gen,
                                                   class_mode = "binary",
                                                   seed = params$seed, 
                                                   target_size = target_size,
                                                   batch_size = batch_size,
                                                   #color_mode = "grayscale",
                                                   subset = "validation")
  
}

test.image_array_gen <- flow_images_from_directory(test.path, 
                                                    test_data_gen,
                                                    class_mode = "binary",
                                                    seed = params$seed,
                                                    target_size = target_size,
                                                    batch_size = 1,
                                                    #color_mode = "grayscale", 
                                                    shuffle = F # Makes it easier to check with 
                                                    # true class labels after predicting. 
                                                    )

# number of training samples
(train_samples <- train.image_array_gen$n)
# number of validation samples
(valid_samples <- val.image_array_gen$n)
# number of testing samples
(test_samples <- test.image_array_gen$n)

# Class index decoding. 
d1 <- cbind(c("effusion", "normal"), 
            c(train.image_array_gen$class_indices$effusion, 
              train.image_array_gen$class_indices$normal))
knitr::kable(d1, caption = "Encoding")
```

The generators yield the batches indefinitely, which means that they act like infinite loops over the images in the specified directories. 

We have a short look at some of the images from the training data set, to get a feel for what the pictures may look like. Notice that the normal class of images is coded as 1 and the effusion class of images is coded as 0. This choice is insignificant for us and is chosen by the image generators. 

```{r}
# Element generate.
batch <- generator_next(train.image_array_gen)
str(batch)

op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in seq(5, 11, 2)) {
  plot(as.raster(batch[[1]][i,,,]), main = paste0("Label: ", batch[[2]][i]))
}
```

Some of the validation images are shown as well, which may be augmented, depending on the choice of the parameter `DA.valid`.

```{r}
# Element generate.
batch <- generator_next(val.image_array_gen)
str(batch)

op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in seq(5, 11, 2)) {
  plot(as.raster(batch[[1]][i,,,]), main = paste0("Label: ", batch[[2]][i]))
}
```

Some of the testing images are shown as well, which have not been augmented. 

```{r}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
  batch <- generator_next(test.image_array_gen)
  plot(as.raster(batch[[1]][1,,,]))
}
```


# Convolutional Neural Network (CNN) Implementation

We have implemented a CNN with three convolutional layers and one fully connected dense hidden layer. This fully connected layer has the same amount of nodes as the image height / width. In between we have added some max pooling layers, some layer dropout and a kernel L2 regularizer in the fully connected dense hidden layer, in order to combat overfitting. 

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                padding="same",
                input_shape = image_size) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu", 
                padding="same") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = img_width, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

# CNN Training

We define the CNN with choice of optimizer, loss function and metric, before we fit the model. We choose the `binary_crossentropy` as loss function, because we have defined the model with one node in the output layer, with sigmoid activation function. Moreover, we choose `rmsprop` as optimizer, because it seems like this gives the greatest performance when comparing to other optimizers, like `adam` and `adadelta`. Finally, we use `accuracy` as the metric, since we want the model to diagnose the images with the greatest accuracy possible. 

After defining the model conveniently, we fit the model. Because the image generators loop endlessly over the images on disk, we need to specify the total number of steps, or batches, before declaring on epoch as finished. Since we are using a generator for both the training data and the validation data, we need to specify this value for both the training data and the validation data. This is done with the arguments `steps_per_epoch` and `validation_steps` below. 

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  #optimizer = optimizer_adam(), # Adam is slow and gives worse performance it seems like. 
  optimizer = optimizer_rmsprop(learning_rate = 1e-4), # decay does not seem to work very well. 
  #optimizer = optimizer_adadelta() # Adadelta is slower and gives worse performance it seems like. 
  metrics = c("accuracy")
)

# We save the model and its state (before fitting), so that 
# it can be used when tuning the hyperparameter. 
model %>% save_model_hdf5("convnet.h5")

history <- model %>% fit(
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
)
plot(history)
```

# Tuning of Hyperparameter

We tune the hyperparameter `batch_size` using the `tfruns` package. This is done exploring the grid `r c(16, 32, 64)` for the values of the hyperparameter. 

This is done in separate files, which are delivered with this report and Rmd. For each of the values in the grid we want to explore, we load the saved model and fit it with the data generators defined using the respective batch size we want to test for. We need to redefine the data generators for each of the three models, because they decide the batch sizes. Notice that this is only implemented for the variant of the solution where data augmentation is performed on both training data and validation data, i.e. when the parameter `params$DA.valid` is set to `TRUE`.

The results from running the file `task2_tfruns.R`, which calls on `task2_explore.R` for each different batch size on the grid, seem to show that a batch size of 64 yields the best results, according to the metric validation accuracy. 

# Early Stopping

We implement early stopping in the model, using the `keras callbacks()` API. The training is interrupted when validation accuracy stops improving (increasing) for more than two epochs. 

```{r}
callback.parameters <- callback_early_stopping(
  monitor = "val_accuracy",
  patience = 2, # This argument is used to interrupt training when
  # validation accuracy stops improving (increasing) for more than two epochs. 
  verbose = 1,
  mode = "max"
)

history <- model %>% fit( 
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # Add callbacks for early stopping. 
  callbacks = callback.parameters
)
plot(history)
```

We notice that it seems like a patience of 2 seems to be a bit strict for this model, since it stops training relatively early. 

# Performance Assessment of CNN

We assess the performance of the CNN by predicting the categories of the test images. The confusion matrix is given.

First we evaluate the model to get an idea of how well the model can predict on the test images. Notice that we, again, have to specify the total number of steps before declaring the evaluation round finished, which is done using the `steps` argument in the `evaluate` function. For the test data generator we have defined the batch size as 1, such that we simply evaluate every test image once during evaluation and prediction. 

```{r}
model %>% evaluate(test.image_array_gen, steps = test_samples)
```

Next we make predictions on every test image. 

```{r}
#test.image_array_gen$reset() # Done in case such that nothing will go wrong. 
y_pred <- model %>% predict(test.image_array_gen, steps = test_samples) %>% 
    `>`(0.5) %>% k_cast("int32")
y_pred <- as.array(y_pred)
confusionMatrix(as.factor(test.image_array_gen$classes), as.factor(y_pred))
```

The confusion matrix shows that the model performs very badly on the test images, rendering it useless in practice. 

We can see that most of the errors the model makes is when predicting normal images (1) as having an effusion (0), i.e. it often predicts effusion on X-ray images that in reality are normal. When predicting images as normal, it makes a lot less mistakes, since most of the predictions are normal according to the true labels. 

# Convolutional Autoencoder (CAE) Implementation

We implement a CAE with 10 nodes in the $z$ layer (the bottleneck). The number of convolutional layers, filter sizes, number of filters are chosen ... 

```{r}
# Based o: https://blog.keras.io/building-autoencoders-in-keras.html
#### Convolutional Encoder
model_enc <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
  activation = "relu", padding = "same",
  input_shape = image_size) %>%
  layer_max_pooling_2d(pool_size = c(2,2), padding = "same") %>% 
  layer_conv_2d(filters = 16, kernel_size = c(3,3),
  activation = "relu", padding = "same") %>%
  layer_max_pooling_2d(pool_size = c(3,3), padding = "same") %>% 
  layer_conv_2d(filters = 8, kernel_size = c(3,3),
  activation = "relu", padding = "same") %>%
  layer_max_pooling_2d(pool_size = c(4,4), strides = c(2,2), 
                       padding = "valid")
summary(model_enc)

#### Convolutional Decoder
model_dec <- keras_model_sequential() %>%
  layer_conv_2d(filters = 8, kernel_size = c(3,3),
  activation = "relu", padding = "valid",
  input_shape = c(10, 10, 8)) %>%
  layer_upsampling_2d(size = c(3,3)) %>%
  layer_conv_2d(filters = 16, kernel_size = c(3,3),
  activation = "relu", padding = "valid") %>% 
  layer_upsampling_2d(size = c(3,3)) %>%
  # Important: no padding
  layer_conv_2d(filters = 1, kernel_size = c(3,3),
  activation = "relu") %>%
  layer_upsampling_2d(size = c(2,2))
summary(model_dec)

model.CAE <- keras_model_sequential() 
# input dimension == output dimension
#### Autoencoder
model.CAE %>% model_enc %>% model_dec
summary(model.CAE)
```

We compile and fit the model. Notice that when fitting this model, we have tried both with and without image augmentation on the validation data as well USIKKER PÅ OM DETTE GIR MENING HER?!

```{r}
model.CAE %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

callback.parameters.CAE <- callback_early_stopping(
  monitor = "mean_squared_error",
  patience = 7, # This argument is used to interrupt training when
  # mean squared error stops improving (decreasing) for more than two epochs. 
  verbose = 1,
  mode = "min"
)

history.CAE <- model.CAE %>% fit( 
  x = train.image_array_gen,
  y = train.image_array_gen,
  
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # Add callbacks for early stopping. 
  callbacks = callback.parameters.CAE
)
plot(history.CAE)
```

We do some predictions. 

RMD KOMPILATOREN FAILA HER!?!? FINN UT HVA FEILEN ER?
```{r}
# Autoencoder
output_cae <- model.CAE %>% predict(test.image_array_gen, steps = test_samples)
#output_cae <- as.array(output_cae)
dim(output_cae)

# Plot the first predicted image as an example. 
plot(as.raster(output_cae[1,,,]))

# Plot the real image. 
plot(as.raster(generator_next(test.image_array_gen)[[1]][1,,,]))
# SVÆRT DÅRLIG MODELL LOL!

# From input to encoder
enc_output <- model_enc %>% predict(test.image_array_gen, steps = test_samples)
dim(enc_output)

# From encoder to decoder
dec_output_cifra <- model_dec %>% predict(enc_output)
dim(dec_output_cifra)¨
```

# Graphical Representation of Results from CAE

We represent graphically the results from the test images to show the association between the activations in the $z$ layer and the class images. DO I USE GRADCAM HERE?? ELLER MENER HAN BARE Å PLOTTE ENCODER RESULTS FOR Å SE HVORDAN BILDENE SER UT ETTER ENCODEREN?

Ta en titt i cifar10_2classes "Visualizing what convnets learn". 
 
# Further Work

As we have seen, neither of the models employed do a very good job in separating between normal X-ray images and the images with effusion. Because of shortage of time close to the end of the spring semester, it was not possible to keep improving these models. I have already tried to combat overfitting problems (especially in the CNN) by using data augmentation in the image generators, adding dropout and L2 kernel regularization. This is something I would have liked to do to a greater extent before delivering this report, but perhaps it can be done after.

HOWEVER, NOTICE ALSO THAT WE DO NOT HAVE A LOT OF DATA (1000 IMAGES). FÅ MED DETTE I DISKUSJONEN OVER OGSÅ, SOM MULIG ÅRSAK TIL AT MODELLENE IKKE ER SÅ VELDIG GODE. 
