---
title: "Task2"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: hide
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 1234
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", warning = F)
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

library(png)
library(imager)
library(tfdatasets)

set.seed(params$seed)
```

The objective of this task is diagnosis of chest X-ray images. More specifically, we want to be able to separate between normal X-ray images and X-ray images with effusion. 

We are given two data sets of images; 500 normal X-ray images and 500 X-ray images with effusion. This data will be used to construct models for diagnosis of future images. 

First we separate the data into two sets; a training set containing 2/3 of the data and a test set containing the remaining 1/3. 

```{r}
set.seed(1234) # Making the setting of the seed more explicit (even though it is done in the setup already).

# We collect the names of all the images. 
normal.image.names <- list.files("./normal")
effusion.image.names <- list.files("./effusion")
```

FUNKER IKKE Å EVALUERE NÅR RMD KOMPILERES!!

```{r, cache = T, eval = F}
# We make a tensor to fit the 500 images, of size 512*512 with 3 color channels. 
normal.images <- array(rep(NA, 500*512*512*3), dim = c(500, 512, 512, 3))
for (i in 1:500){
  normal.images[i,,,] <- readPNG(paste0("normal/", normal.image.names[i]))
  # This readPNG function could perhaps be replaced for load.image for Imager packager if that is better!
}

# We make a tensor to fit the 500 images, of size 512*512 with 3 color channels. 
effusion.images <- array(rep(NA, 500*512*512*3), dim = c(500, 512, 512, 3))
for (i in 1:500){
  effusion.images[i,,,] <- readPNG(paste0("effusion/", effusion.image.names[i]))
  # This readPNG function could perhaps be replaced for load.image for Imager packager if that is better!
}

# I AM USING ALL MY RAM FOR THIS, THIS IS NOT A VIABLE OPTION!! HOW ELSE CAN IT BE DONE?
```


```{r, eval = F}
# We plot one normal image to check that it has worked. 
img <- normal.images[1,,,]
plot(as.raster(img), main = "Normal X-ray Example", ylab = "", xlab = "")
#rasterImage(img, 1.2, 1.0, 1.8, 2.0)

# Similarly, we plot one effusion X-ray. 
img <- effusion.images[1,,,]
plot(as.raster(img), main = "Effusion X-ray Example", ylab = "", xlab = "")
#rasterImage(img, 1.2, 1.0, 1.8, 2.0)
```

```{r, eval = F, results='hide'}
# We load the files as a TensorFlow Dataset, following the guide
# https://tensorflow.rstudio.com/tutorials/beginners/load/load_image/
# This seems like a way better way of doing it, because it uses an iterator to only load one and one image!

normal.list.ds <- file_list_dataset("./normal/*")
effusion.list.ds <- file_list_dataset("./effusion/*")

decode_img <- function(file_path, height = 512, width = 512) {
  
  size <- as.integer(c(height, width))
  
  file_path %>% 
    tf$io$read_file() %>% 
    tf$image$decode_jpeg(channels = 3) %>% 
    tf$image$convert_image_dtype(dtype = tf$float64) %>% 
    tf$image$resize(size = size)
}

normal.preprocess_path <- function(file_path) {
  list(
    decode_img(file_path),
    to_categorical(TRUE)
  )
}

effusion.preprocess_path <- function(file_path) {
  list(
    decode_img(file_path),
    to_categorical(FALSE) # This does not make sense the way I have done it here, since it is all FALSE. 
  )
}

normal.labeled.ds <- normal.list.ds %>% 
  dataset_map(normal.preprocess_path, num_parallel_calls = tf$data$experimental$AUTOTUNE)

effusion.labeled.ds <- effusion.list.ds %>% 
  dataset_map(effusion.preprocess_path, num_parallel_calls = tf$data$experimental$AUTOTUNE)

check.normal <- normal.labeled.ds %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next() 

check.normal
plot(as.raster(as.array(check.normal[[1]])))


check.effusion <- effusion.labeled.ds %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next() 

check.effusion
plot(as.raster(as.array(check.effusion[[1]])))
```

The data is split below. 

```{r, eval = F}
train.indices <- sample(1:500, size = 2/3*500)
normal.train <- normal.images[train.indices,,,]
normal.test <- normal.images[-train.indices,,,]

effusion.train <- effusion.images[train.indices,,,]
effusion.test <- effusion.images[-train.indices,,,]
# Missing labels here also. 

# Used when loading all images to RAM, which should not be done!!
gc() # Run garbage collection to remove unused variables (to restore RAM). 
```

# Convolutional Neural Network (CNN) Implementation

We implement a CNN with the following architecture. 

```{r, eval = F}
model <- keras_model_sequential() %>%
layer_conv_2d(filters=32, kernel_size=3, activation = "relu", input_shape=c(64, 64, 3)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=64, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(model)
```

DOING SOME TESTING WITH THE ITERATOR DATA SET, FOLLOWING THE GUIDE FROM TENSORFLOW I LINKED TO ABOVE. DOES NOT WORK. 

```{r, eval = F}
model <- keras_model_sequential() %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")

model %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
  )


# Does not work to fit with this "iterator data". Not sure how to solve the problem I am having with importing the data tbh!
model %>% 
  fit(
    normal.labeled.ds %>% dataset_prefetch(buffer_size = tf$data$experimental$AUTOTUNE),
    epochs = 5,
    verbose = 2
  )
```

# CNN Training

We define the CNN with choice of optimizer, loss function, metrics.etc. DETTE BØR NOK FINSKRIVES NOE!!

```{r, eval = F}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 100,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callback.parameters
)
plot(history)
```


# Tuning of Hyperparameter

We tune the hyperparameter `batch_size` using the `tfruns` package. This is done exploring the grid `r c(16, 32, 64)` for the values of the hyperparameter. 

# Early Stopping

We implement early stopping in the model, using the `keras callbacks()` API. The training is interrupted when validation accuracy stops improving for more than two epochs. 

```{r, eval = F}
callback.parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 2, # This argument is used to interrupt training when validation accuracy stops improving for more than two epochs. 
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

history <- model %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 100,
  batch_size = batch.size,
  validation_split = 0.2,
  callbacks = callback.parameters
)
plot(history)
```


# Performance Assessment

We assess the performance of the CNN by predicting the categories of the test images. The confusion matrix is given. 

```{r, eval = F}
# keras/tensorflow version >= 2.6
# se obtiene un objeto tf.tensor
y_pred <- model %>% predict(x_test) %>% k_argmax()
# se pasa a vector
# https://tensorflow.rstudio.com/guide/tensorflow/tensors/
y_pred <- y_pred %>% shape() %>% unlist()
confusionMatrix(as.factor(DATATESTLABELS), as.factor(y_pred))
```


# Convolutional Autoencoder (CAE) Implementation

We implement a CAE with 10 nodes in the $z$ layer (the bottleneck). The number of convolutional layers, filter sizes, number of filters.etc are chosen ... 

```{r, eval = F}
# Based o: https://blog.keras.io/building-autoencoders-in-keras.html
#### Convolutional Encoder
model_enc <- keras_model_sequential()
model_enc %>%
layer_conv_2d(filters = 16, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = input_dim) %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
summary(model_enc)

#### Convolutional Decoder
model_dec <- keras_model_sequential()
model_dec %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = c(4, 4, 8)) %>%
layer_upsampling_2d(size = c(2,2)) %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same")
layer_upsampling_2d(size = c(2,2)) %>%
# Important: no padding
layer_conv_2d(filters = 1, kernel_size = c(3,3),
activation = "relu") %>%
layer_upsampling_2d(size = c(2,2))
summary(model_dec)

model.CAE <- keras_model_sequential() 
# input dimension == output dimension
#### Autoencoder
model.CAE %>% model_enc %>% model_dec
summary(model.CAE)
```

We compile and fit the model. 

```{r, eval = F}
model %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

history <- model %>% fit(
  x= x_train_cifra, y = x_train_cifra,
  # Autoencoder
  epochs = 5, batch_size = 128,
  suffle = TRUE,
  validation_split = 0.2
)
```

We do predictions. 

```{r, eval = F}
# Autoencoder
output_cifra <- predict(model,x_test_cifra)
dim(output_cifra)

# From input to encoder
enc_output_cifra<-predict(model_enc,x_test_cifra)
dim(enc_output_cifra)

# From encoder to decoder
dec_output_cifra<-predict(model_dec,enc_output_cifra)
dim(dec_output_cifra)
```


# Graphical Representation of Results

We represent graphically the results from the test images to show the association between the activations in the $z$ layer and the class images. DO I USE GRADCAM HERE??
 