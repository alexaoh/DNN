---
title: "Task2"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: hide
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 1234
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", warning = F)
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

library(png)
#library(imager)
#library(tfdatasets)

set.seed(params$seed)
```

The objective of this task is diagnosis of chest X-ray images. More specifically, we want to be able to separate between normal X-ray images and X-ray images with effusion. 

We are given two data sets of images; 500 normal X-ray images and 500 X-ray images with effusion. This data will be used to construct models for diagnosis of future images. 

First we separate the data into two sets; a training set containing 2/3 of the data and a test set containing the remaining 1/3. 

```{r}
set.seed(1234) # Making the setting of the seed more explicit (even though it is done in the setup already).

image.path <- "./images"

# We collect the names of all the images. 
normal.image.names <- list.files(paste0(image.path,"/normal"))
effusion.image.names <- list.files(paste0(image.path,"/effusion"))
```

```{r, cache = T}
img.width <- 512
img.height <- 512
# We make two tensors to fit the 500 images of both types respectively, of size 512*512 with 1 color channel. 
normal.images <- effusion.images <- array(rep(NA, 500*img.width*img.height), dim = c(500, img.width, img.height))
for (i in 1:500){
  normal.images[i,,] <- readPNG(paste0(image.path, "/normal/",normal.image.names[i]))[,,1]
  # readPNG assumes RGB images. Since ours are greyscale, R = G = B, and we can remove to color channels, which is why we have [,,1].
  # Images are too big to train the model with images in memory!! Need to reshape or go back to generators!
  # or use image_load from Keras to choose grayscale and target_size, but I do not know how to plot these e.g.
}

# We make a tensor to fit the 500 images, of size 512*512 with 1 color channel. 
for (i in 1:500){
  effusion.images[i,,] <- readPNG(paste0(image.path, "/effusion/", effusion.image.names[i]))[,,1]
  # readPNG assumes RGB images. Since ours are greyscale, R = G = B, and we can remove to color channels, which is why we have [,,1].
}

# Must be some better way than to simply load it into memory? There are iterators etc, but have not completely figured them out yet!
```


```{r}
# We plot one normal image to check that it has worked. 
img <- normal.images[1,,]
plot(as.raster(img), main = "Normal X-ray Example", ylab = "", xlab = "")
#rasterImage(img, 1.2, 1.0, 1.8, 2.0)

# Similarly, we plot one effusion X-ray. 
img <- effusion.images[1,,]
plot(as.raster(img), main = "Effusion X-ray Example", ylab = "", xlab = "")
#rasterImage(img, 1.2, 1.0, 1.8, 2.0)
```

The data is split below. We select $\frac{2}{3}$ of each of the respective types of images for training, while the remaining data is used for testing. 

```{r, cache = T}
part <- 2/3
num.images <- 500
train.indices <- sample(1:num.images, size = part*num.images)
normal.train <- normal.images[train.indices,,]
normal.test <- normal.images[-train.indices,,]

effusion.train <- effusion.images[train.indices,,]
effusion.test <- effusion.images[-train.indices,,]
# Missing labels here also. 

# Used when loading all images to RAM, which should not be done!!
#gc() # Run garbage collection to remove unused variables (to restore RAM). 

x.train <- k_concatenate(list(normal.train, effusion.train), axis = 1)
# Label "normal" as 0 and "effusion" as 1.
#y.train <- to_categorical(c(rep(0, part*num.images), rep(1, part*num.images))) # for softmax and two output nodes. 
y.train <- c(rep(0, part*num.images), rep(1, part*num.images))

x.test <- k_concatenate(list(normal.test, effusion.test), axis = 1)
# Label "normal" as 0 and "effusion" as 1.
#y.test <- to_categorical(c(rep(0, num.images-round(part*num.images)), rep(1, num.images-round(part*num.images)))) # for softmax and two output nodes. 
y.test <- c(rep(0, num.images-round(part*num.images)), rep(1, num.images-round(part*num.images)))

gc() # Run Garbage Collection to free up memory from unused variables.
```

# Convolutional Neural Network (CNN) Implementation

We implement a CNN with the following architecture. 

```{r}
model <- keras_model_sequential() %>%
layer_conv_2d(filters=32, kernel_size=3, activation = "relu", input_shape=c(img.width, img.height, 1)) %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=64, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters=128, kernel_size=3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
summary(model)
```

# CNN Training

We define the CNN with choice of optimizer, loss function, metrics.etc. DETTE BÃ˜R NOK FINSKRIVES NOE!!

```{r}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  x = x.train,
  y = y.train,
  epochs = 100,
  batch_size = 64,
  validation_split = 0.2
  
  #,callbacks = callback.parameters
)
plot(history)
```

# Tuning of Hyperparameter

We tune the hyperparameter `batch_size` using the `tfruns` package. This is done exploring the grid `r c(16, 32, 64)` for the values of the hyperparameter. 

# Early Stopping

We implement early stopping in the model, using the `keras callbacks()` API. The training is interrupted when validation accuracy stops improving for more than two epochs. 

```{r, eval = F}
callback.parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 2, # This argument is used to interrupt training when validation accuracy stops improving for more than two epochs. 
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

history <- model %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 100,
  batch_size = batch.size,
  validation_split = 0.2,
  callbacks = callback.parameters
)
plot(history)
```


# Performance Assessment

We assess the performance of the CNN by predicting the categories of the test images. The confusion matrix is given. 

```{r, eval = F}
# keras/tensorflow version >= 2.6
# se obtiene un objeto tf.tensor
y_pred <- model %>% predict(x_test) %>% k_argmax()
# se pasa a vector
# https://tensorflow.rstudio.com/guide/tensorflow/tensors/
y_pred <- y_pred %>% shape() %>% unlist()
confusionMatrix(as.factor(DATATESTLABELS), as.factor(y_pred))
```


# Convolutional Autoencoder (CAE) Implementation

We implement a CAE with 10 nodes in the $z$ layer (the bottleneck). The number of convolutional layers, filter sizes, number of filters.etc are chosen ... 

```{r, eval = F}
# Based o: https://blog.keras.io/building-autoencoders-in-keras.html
#### Convolutional Encoder
model_enc <- keras_model_sequential()
model_enc %>%
layer_conv_2d(filters = 16, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = input_dim) %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
summary(model_enc)

#### Convolutional Decoder
model_dec <- keras_model_sequential()
model_dec %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = c(4, 4, 8)) %>%
layer_upsampling_2d(size = c(2,2)) %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same")
layer_upsampling_2d(size = c(2,2)) %>%
# Important: no padding
layer_conv_2d(filters = 1, kernel_size = c(3,3),
activation = "relu") %>%
layer_upsampling_2d(size = c(2,2))
summary(model_dec)

model.CAE <- keras_model_sequential() 
# input dimension == output dimension
#### Autoencoder
model.CAE %>% model_enc %>% model_dec
summary(model.CAE)
```

We compile and fit the model. 

```{r, eval = F}
model %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

history <- model %>% fit(
  x= x_train_cifra, y = x_train_cifra,
  # Autoencoder
  epochs = 5, batch_size = 128,
  suffle = TRUE,
  validation_split = 0.2
)
```

We do predictions. 

```{r, eval = F}
# Autoencoder
output_cifra <- predict(model,x_test_cifra)
dim(output_cifra)

# From input to encoder
enc_output_cifra<-predict(model_enc,x_test_cifra)
dim(enc_output_cifra)

# From encoder to decoder
dec_output_cifra<-predict(model_dec,enc_output_cifra)
dim(dec_output_cifra)
```


# Graphical Representation of Results

We represent graphically the results from the test images to show the association between the activations in the $z$ layer and the class images. DO I USE GRADCAM HERE??
 