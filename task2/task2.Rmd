---
title: "Task2"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: hide
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 1234
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", warning = F)
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

library(png)
#library(imager)
#library(tfdatasets)

set.seed(params$seed)
```

The objective of this task is diagnosis of chest X-ray images. More specifically, we want to be able to separate between normal X-ray images and X-ray images with effusion. 

We are given two data sets of images; 500 normal X-ray images and 500 X-ray images with effusion. This data will be used to construct models for diagnosis of future images. 

First we separate the data into two sets; a training set containing 2/3 of the data and a test set containing the remaining 1/3. 

```{r}
set.seed(1234) # Making the setting of the seed more explicit (even though it is done in the setup already).
img_width <- 64 # or 256, 128, 64 or 32 (try different sizes)!
img_height <- 64
target_size <- c(img_width, img_height)
batch_size <- 32
epochs <- 30
channels <- 3 # RGB = 3 channels
image_size <- c(target_size, channels)

# optional data augmentation.
train_data_gen <- image_data_generator(
  rescale = 1/255, 
  validation_split = 1/3,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

# Then we load the data using the generators. 
# training images.
train.image_array_gen <- flow_images_from_directory("images", 
                                                    train_data_gen,
                                                    class_mode = "binary",
                                                    seed = 1234, # Change to params$seed in Rmd.
                                                    target_size = target_size,
                                                    batch_size = batch_size,
                                                    #color_mode = "grayscale",
                                                    subset = "training")

# testing (or possibly validation) images. 
val.image_array_gen <- flow_images_from_directory("images", 
                                                   train_data_gen,
                                                   class_mode = "binary",
                                                   seed = 1234, # Change to params$seed in Rmd.
                                                   target_size = target_size,
                                                   batch_size = batch_size,
                                                   #color_mode = "grayscale",
                                                   subset = "validation")

# number of training samples
train_samples <- train.image_array_gen$n
# number of validation samples
valid_samples <- val.image_array_gen$n

# Element generate
batch <- generator_next(train.image_array_gen)

str(batch)

```

We plot some of the loaded images, to see what they look like. 

```{r}
# Vi plotter bildene for å se hvordan data augmentation fungerer!
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
  plot(as.raster(batch[[1]][i,,,]))
}

```

The data is split below. We select $\frac{2}{3}$ of each of the respective types of images for training, while the remaining data is used for testing. 

```{r, cache = T}
# Hvordan kan jeg splitte når jeg bruker generatorene?!
```

# Convolutional Neural Network (CNN) Implementation

We implement a CNN with the following architecture. 

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                padding="same",
                input_shape = image_size) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu", 
                padding="same") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

We have implemented a CNN with three concolutional layers and one fully connected dense hidden layer. In between we have added some max pooling layers, some layer dropout and a kernel L2 regularizer in the fully connected dense hidden layer, in order to combat overfitting. TESTER ANDRE MODELLER I "importing_images.R".

# CNN Training

We define the CNN with choice of optimizer, loss function, metrics.etc. DETTE BØR NOK FINSKRIVES NOE!!

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  #optimizer = "adam", # Adam fungerer dårlig her virker det som på meg!
  optimizer = optimizer_rmsprop(learning_rate = 1e-4), # decay does not seem to work very well. 
  #optimizer = optimizer_adadelta(),
  metrics = c("accuracy")
)
history <- model %>% fit( # Or fit_generator?
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
)
plot(history)

#model %>% save_model_hdf5(paste0("cifar10_",i,"_",j,"_finetuningtransferlearning.h5"))
# Hva gjør egentlig denne? Lagrer med ferdige trente weights og alt? Søk opp i docs når R er ledig!
```

# Tuning of Hyperparameter

We tune the hyperparameter `batch_size` using the `tfruns` package. This is done exploring the grid `r c(16, 32, 64)` for the values of the hyperparameter. 

# Early Stopping

We implement early stopping in the model, using the `keras callbacks()` API. The training is interrupted when validation accuracy stops improving for more than two epochs. 

```{r, eval = F}
callback.parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 2, # This argument is used to interrupt training when validation accuracy stops improving for more than two epochs. 
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

history <- model %>% fit( # Or fit_generator?
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # Add callbacks for early stopping. 
  callbacks = callback.parameters
)
plot(history)
```


# Performance Assessment

We assess the performance of the CNN by predicting the categories of the test images. The confusion matrix is given. 

```{r, eval = F}
# keras/tensorflow version >= 2.6
# se obtiene un objeto tf.tensor
y_pred <- model %>% predict(x_test) %>% k_argmax()
# se pasa a vector
# https://tensorflow.rstudio.com/guide/tensorflow/tensors/
y_pred <- y_pred %>% shape() %>% unlist()
confusionMatrix(as.factor(DATATESTLABELS), as.factor(y_pred))
```


# Convolutional Autoencoder (CAE) Implementation

We implement a CAE with 10 nodes in the $z$ layer (the bottleneck). The number of convolutional layers, filter sizes, number of filters.etc are chosen ... 

```{r, eval = F}
# Based o: https://blog.keras.io/building-autoencoders-in-keras.html
#### Convolutional Encoder
model_enc <- keras_model_sequential()
model_enc %>%
layer_conv_2d(filters = 16, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = input_dim) %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
summary(model_enc)

#### Convolutional Decoder
model_dec <- keras_model_sequential()
model_dec %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = c(4, 4, 8)) %>%
layer_upsampling_2d(size = c(2,2)) %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same")
layer_upsampling_2d(size = c(2,2)) %>%
# Important: no padding
layer_conv_2d(filters = 1, kernel_size = c(3,3),
activation = "relu") %>%
layer_upsampling_2d(size = c(2,2))
summary(model_dec)

model.CAE <- keras_model_sequential() 
# input dimension == output dimension
#### Autoencoder
model.CAE %>% model_enc %>% model_dec
summary(model.CAE)
```

We compile and fit the model. 

```{r, eval = F}
model %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

history <- model %>% fit(
  x= x_train_cifra, y = x_train_cifra,
  # Autoencoder
  epochs = 5, batch_size = 128,
  suffle = TRUE,
  validation_split = 0.2
)
```

We do predictions. 

```{r, eval = F}
# Autoencoder
output_cifra <- predict(model,x_test_cifra)
dim(output_cifra)

# From input to encoder
enc_output_cifra<-predict(model_enc,x_test_cifra)
dim(enc_output_cifra)

# From encoder to decoder
dec_output_cifra<-predict(model_dec,enc_output_cifra)
dim(dec_output_cifra)
```


# Graphical Representation of Results

We represent graphically the results from the test images to show the association between the activations in the $z$ layer and the class images. DO I USE GRADCAM HERE??

Ta en titt i cifar10_2classes "Visualizing what convnets learn". 
 