---
title: "Task2"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: hide
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 1234
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>", warning = F)
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

library(png)
#library(imager)
library(tfdatasets)

set.seed(params$seed)
```

The objective of this task is diagnosis of chest X-ray images. More specifically, we want to be able to separate between normal X-ray images and X-ray images with effusion. 

We are given two data sets of images; 500 normal X-ray images and 500 X-ray images with effusion. This data will be used to construct models for diagnosis of future images. 

# Preprocessing

First we separate the data into two sets; a training set containing 2/3 of the data and a test set containing the remaining 1/3. In order to do this, we assume that both sets of images are saved in a directory called "images". This directory contains the 500 normal X-ray images in a sub directory called "normal", and the 500 X-ray images with diffusion in a sub directory called "effusion". The images are given in png format. 

We create two new sub directories using within the "images" directory; one called "test", which contains the randomly sampled test data, and one called "train", which contains the randomly sampled training data. We assume that these directories have not been created on the computer earlier. 

```{r}
set.seed(params$seed) # Set seed to 1234, as chosen in params. 
image.path <- "./images"
normal.path <- paste0(image.path, "/normal")
effusion.path <- paste0(image.path, "/effusion") 
train.path <- paste0(image.path, "/train") # New path for training images. 
test.path <- paste0(image.path, "/test") # New path for test images. 
train.normal.path <- paste0(train.path, "/normal") 
train.effusion.path <- paste0(train.path, "/effusion")
test.normal.path <- paste0(test.path, "/normal") 
test.effusion.path <- paste0(test.path, "/effusion")

# We collect the names of all the images.
normal.image.names <- list.files(normal.path)
effusion.image.names <- list.files(effusion.path)

# We create the two new directories. If they already exist, a warning is printed. 
dir.create(train.path)
dir.create(test.path)

# We also create "effusion" and "normal" sub directories inside the new "train" and "test" directories. 
# This is needed to get images for models later. 
dir.create(train.normal.path)
dir.create(test.normal.path)
dir.create(train.effusion.path)
dir.create(test.effusion.path)

# We fill the newly created sub folders with randomly sampled data. 
# We know that the images are named (for example) "normal0.png" and "effusion0.png", 
# with numbers ranging from 0 to 499. We check that this is the case below. 
all(sort(parse_number(normal.image.names)) == 0:499)
all(sort(parse_number(effusion.image.names)) == 0:499)

# We sample from 0:499, in order to choose training numbers.
train.numbers <- sample(0:499, size = 2/3*500)
# Find testing numbers as well, simply using a set difference. 
test.numbers <- setdiff(0:499, train.numbers)

# Next we select the image names corresponding to the indices sampled above. 
normal.image.names <- data.frame(normal.image.names)
effusion.image.names <- data.frame(effusion.image.names)

# Select image names corresponding to training numbers sampled above. 
train.normal <- normal.image.names %>% dplyr::filter(parse_number(normal.image.names) %in% train.numbers)
train.normal <- train.normal$normal.image.names # Change data type of names to vector. 
all(sort(parse_number(train.normal)) == sort(train.numbers)) # Check that we have selected the correct names.

test.normal <- normal.image.names %>% dplyr::filter(parse_number(normal.image.names) %in% test.numbers)
test.normal <- test.normal$normal.image.names # Change data type of names to vector. 
all(sort(parse_number(test.normal)) == sort(test.numbers)) # Check that we have selected the correct names.

# We sample from 0:499, in order to choose training numbers. We do it again for the effusion images, 
# in case the indices of the two types of images are not chosen randomly (we do not know how the images are named by the source).
train.numbers <- sample(0:499, size = 2/3*500)
# Find testing numbers as well, simply using a set difference. 
test.numbers <- setdiff(0:499, train.numbers)

# Next we select the image names corresponding to the indices sampled above. 
normal.image.names <- data.frame(normal.image.names)
effusion.image.names <- data.frame(effusion.image.names)

train.effusion <- effusion.image.names %>% dplyr::filter(parse_number(effusion.image.names) %in% train.numbers)
train.effusion <- train.effusion$effusion.image.names # Change data type of names to vector. 
all(sort(parse_number(train.effusion)) == sort(train.numbers)) # Check that we have selected the correct names.

test.effusion <- effusion.image.names %>% dplyr::filter(parse_number(effusion.image.names) %in% test.numbers)
test.effusion <- test.effusion$effusion.image.names # Change data type of names to vector. 
all(sort(parse_number(test.effusion)) == sort(test.numbers)) # Check that we have selected the correct names.


# Now that we have all the file names, we copy them into their respective directories. 
#apply(train.normal, FUN = file.copy, from = paste0(image.path,"/normal"), to = paste0(image.path, "/train"))
for (i in 1:length(train.numbers)){
  name <- train.normal[i]
  file.copy(from = paste0(normal.path, "/", name), to = train.normal.path)
  
  name <- train.effusion[i]
  file.copy(from = paste0(effusion.path, "/", name), to = train.effusion.path)
}

# Må lage subdirectories per class!!

for (i in 1:length(test.numbers)){
  name <- test.normal[i] 
  file.copy(from = paste0(normal.path, "/", name), to = test.normal.path)
  
  name <- test.effusion[i]
  file.copy(from = paste0(effusion.path, "/", name), to = test.effusion.path)
}

# Check that it worked!
train.normal.new <- list.files(train.normal.path)
length(train.normal.new)
all(train.normal.new == train.normal)
train.effusion.new <- list.files(train.effusion.path)
length(train.effusion.new)
all(train.effusion.new == train.effusion)
test.normal.new <- list.files(test.normal.path)
length(test.normal.new)
all(test.normal.new == test.normal)
test.effusion.new <- list.files(test.effusion.path)
length(test.effusion.new)
all(test.effusion.new == test.effusion)
```

# Image Data Generators 

We make image data generators in order to work with models. 

```{r}
img_width <- 64 # or 256, 128, 64 or 32 (try different sizes)!
img_height <- 64
target_size <- c(img_width, img_height)
batch_size <- 32
epochs <- 30
channels <- 3 # RGB = 3 channels
image_size <- c(target_size, channels)

# optional data augmentation.
train_data_gen <- image_data_generator(
  rescale = 1/255, 
  validation_split = 1/3,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

# Then we load the data using the generators. 
# training images.
train.image_array_gen <- flow_images_from_directory(train.path, 
                                                    train_data_gen,
                                                    class_mode = "binary",
                                                    seed = 1234, # Change to params$seed in Rmd.
                                                    target_size = target_size,
                                                    batch_size = batch_size,
                                                    #color_mode = "grayscale",
                                                    subset = "training")

# validation images. 
val.image_array_gen <- flow_images_from_directory(train.path, 
                                                   train_data_gen,
                                                   class_mode = "binary",
                                                   seed = 1234, # Change to params$seed in Rmd.
                                                   target_size = target_size,
                                                   batch_size = batch_size,
                                                   #color_mode = "grayscale",
                                                   subset = "validation")

test.image_array_gen <- flow_images_from_directory(test.path, 
                                                    train_data_gen,
                                                    class_mode = "binary",
                                                    seed = 1234, # Change to params$seed in Rmd.
                                                    target_size = target_size,
                                                    batch_size = batch_size,
                                                    #color_mode = "grayscale"
                                                    )

# number of training samples
train_samples <- train.image_array_gen$n
# number of validation samples
valid_samples <- val.image_array_gen$n
# number of testing samples
test_samples <- test.image_array_gen$n

# Element generate
batch <- generator_next(train.image_array_gen)
str(batch)

```

We plot some of the loaded images, to see what they look like. 

```{r}
# We plot some of the training images to see what they look like. 
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
  plot(as.raster(batch[[1]][i,,,]))
}

```

# Convolutional Neural Network (CNN) Implementation

We implement a CNN with the following architecture. 

```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", 
                padding="same",
                input_shape = image_size) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu", 
                padding="same") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

We have implemented a CNN with three concolutional layers and one fully connected dense hidden layer. In between we have added some max pooling layers, some layer dropout and a kernel L2 regularizer in the fully connected dense hidden layer, in order to combat overfitting. TESTER ANDRE MODELLER I "importing_images.R".

# CNN Training

We define the CNN with choice of optimizer, loss function, metrics.etc. DETTE BØR NOK FINSKRIVES NOE!!

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  #optimizer = "adam", # Adam fungerer dårlig her virker det som på meg!
  optimizer = optimizer_rmsprop(learning_rate = 1e-4), # decay does not seem to work very well. 
  #optimizer = optimizer_adadelta(),
  metrics = c("accuracy")
)
history <- model %>% fit( # Or fit_generator?
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
)
plot(history)

#model %>% save_model_hdf5(paste0("cifar10_",i,"_",j,"_finetuningtransferlearning.h5"))
# Hva gjør egentlig denne? Lagrer med ferdige trente weights og alt? Søk opp i docs når R er ledig!
```

# Tuning of Hyperparameter

We tune the hyperparameter `batch_size` using the `tfruns` package. This is done exploring the grid `r c(16, 32, 64)` for the values of the hyperparameter. 

# Early Stopping

We implement early stopping in the model, using the `keras callbacks()` API. The training is interrupted when validation accuracy stops improving for more than two epochs. 

```{r, eval = F}
callback.parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 2, # This argument is used to interrupt training when validation accuracy stops improving for more than two epochs. 
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

history <- model %>% fit( # Or fit_generator?
  train.image_array_gen,
  # epochs
  steps_per_epoch = as.integer(train_samples / batch_size), 
  epochs = epochs, 
  
  # validation data
  validation_data = val.image_array_gen,
  validation_steps = as.integer(valid_samples / batch_size),
  
  # Add callbacks for early stopping. 
  callbacks = callback.parameters
)
plot(history)
```


# Performance Assessment

We assess the performance of the CNN by predicting the categories of the test images. The confusion matrix is given. 

```{r, eval = F}
# keras/tensorflow version >= 2.6
# se obtiene un objeto tf.tensor
y_pred <- model %>% predict(x_test) %>% k_argmax()
# se pasa a vector
# https://tensorflow.rstudio.com/guide/tensorflow/tensors/
y_pred <- y_pred %>% shape() %>% unlist()
confusionMatrix(as.factor(DATATESTLABELS), as.factor(y_pred))
```


# Convolutional Autoencoder (CAE) Implementation

We implement a CAE with 10 nodes in the $z$ layer (the bottleneck). The number of convolutional layers, filter sizes, number of filters.etc are chosen ... 

```{r, eval = F}
# Based o: https://blog.keras.io/building-autoencoders-in-keras.html
#### Convolutional Encoder
model_enc <- keras_model_sequential()
model_enc %>%
layer_conv_2d(filters = 16, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = input_dim) %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same") %>%
layer_max_pooling_2d(pool_size = c(2,2), padding = "same")
summary(model_enc)

#### Convolutional Decoder
model_dec <- keras_model_sequential()
model_dec %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same",
input_shape = c(4, 4, 8)) %>%
layer_upsampling_2d(size = c(2,2)) %>%
layer_conv_2d(filters = 8, kernel_size = c(3,3),
activation = "relu", padding = "same")
layer_upsampling_2d(size = c(2,2)) %>%
# Important: no padding
layer_conv_2d(filters = 1, kernel_size = c(3,3),
activation = "relu") %>%
layer_upsampling_2d(size = c(2,2))
summary(model_dec)

model.CAE <- keras_model_sequential() 
# input dimension == output dimension
#### Autoencoder
model.CAE %>% model_enc %>% model_dec
summary(model.CAE)
```

We compile and fit the model. 

```{r, eval = F}
model %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

history <- model %>% fit(
  x= x_train_cifra, y = x_train_cifra,
  # Autoencoder
  epochs = 5, batch_size = 128,
  suffle = TRUE,
  validation_split = 0.2
)
```

We do predictions. 

```{r, eval = F}
# Autoencoder
output_cifra <- predict(model,x_test_cifra)
dim(output_cifra)

# From input to encoder
enc_output_cifra<-predict(model_enc,x_test_cifra)
dim(enc_output_cifra)

# From encoder to decoder
dec_output_cifra<-predict(model_dec,enc_output_cifra)
dim(dec_output_cifra)
```


# Graphical Representation of Results

We represent graphically the results from the test images to show the association between the activations in the $z$ layer and the class images. DO I USE GRADCAM HERE??

Ta en titt i cifar10_2classes "Visualizing what convnets learn". 
 