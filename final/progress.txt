In this file I describe the progress I am making, such that it can easily be pasted into a report after working for a little while. 

PART 1 - Making the CNN

I downloaded the Kaggle dataset. Via the zip file I manually extracted the first 50 folders of train and test images (different types of birds), and saved the folders in a directory called "train" and "test" respectively. I checked that all the bird types were equal in both the directories, which seemed correct. Thus, we have our two datasets necessary to continue with the project.

After having the data ready, we read all bird names into R by listing the subfolder names of the train directory. Moreover, we check that the subfolder names are the same when reading from train and test directories, just to double check that we have selected the same folders from the Kaggle download. This is true. We save the label names to disk for convenience. 

Next we define the image width and height, defining the target size of the images. We also define the number of color channels, which is 3 in this case, since the images are RGB. Moreover, we define the batch size which will be used during the fitting and the epochs. 

In order to work efficiently with the images, we use image data generators from Keras. In these we also define the preprocessing of the images and make a validation split from the test data. We simply rescale the images to have pixel values within [0,1] and set the validation split to 0.2. We do not apply any further image augmentation techniques. Then we use the flow_images_from_directory() function to automatically import batches of images. We do this for all three data sets - training, validation and testing. 

We use a CNN to classify the images. Instead of defining our own model, we load a pretrained model to quickly get acceptable baseline results. In this case we load the xception network with the weights pre-trained on the ImageNet dataset. We freeze all weights in this pre-trained model, but couple it with another convolutional layer, which has weights that will be trained by us on our specific problem.    

We train the model for the first time and save the entire model image. Notice that I was not able to install the tensorflow backend onto my NVIDIA GPU, which meant that the training process was very slow. Thus, instead of training it on my computer, effectively disabilitating it for a few hours, I trained the models on a large computer located at my home university, via ssh. On this computer the process is parallelized, meaning that it is trained faster. Moreover, since I was using tmux, which is a terminal multiplexer, I was able to detach the session and log out of the computer without the computations stopping. The reason I saved the entire model image was so that I could copy it to my computer (via scp), load it into my file and make predictions. After loading the fitted model, we evaluate the model on the test data, in order to see how well it has performed. 84% for the first model is the accuracy given, which is not bad for the first model, before tuning the hyperparameters. Then we predict on another image, which has not been used in training, taken from the Wikipedia page of Abbot's babbler, which is one of the bird species we have trained our model on. An overview of the model's predictions is made, showing the top 5 most probable bird species according to the model. The model gives the majority of the probability density (approximately 72%) to Abbot's babbler, meaning that it classifies the bird correctly, with some certainty. So far, not bad for a first model. 

After this we have a look at which birds are well identified up against the birds that are not well identified. We can see that over half of the species are 100% correctly classified by our model in our test data. However, there are a few species that are mostly misclassified or not classified correctly at all, for instance "Apapane", which is never classified correctly in our test set. 

We want to improve the performance of the model by tuning some of the hyperparameters. In the guide he used a brute force, self-made approach, using simple for-loops over the parameters he wanted to test. I will use tfruns, which is very similar, but seems slightly more sofisticated compared to his approach. This was also run on the computer located at my home university, naturally taking a longer time than the original computations, since a lot more models will be trained. We explore the same grid of hyperparameters as the author. 

PART 2 - Making the Shiny app

Copy the final model we trained into the "www" subdirectory of the "birdapp" directory. We also copy the label list, i.e. the list of birds, into the subdirectory. 

First we define the ui object (User Interface). We use the dashboardPage function to create a dashboard page for the Shiny app. Inside the dashboardPage we define a dashboardHeader, a dashboardSidebar and a dashboardBody. Next we create the server object, which contains the interactive elements of the app. Inside the server function we load the image that is uploaded to the webapp by the user. Then we use the model we trained earlier to predict on the newly uploaded image. We create a dataframe with the top five predicted bird species after prediction, similar to what we did when testing the model earlier. Then we render it to the ui as a table, using the renderTable function. A warning text is defined, which display a warning to the visitor if the model has highest predicted probability below 45%. Finally we display the image that was loaded from the user and delete the file from memory. 
