"","run_dir","metric_val_accuracy","metric_loss","metric_accuracy","metric_val_loss","flag_learning_rate","flag_dropout_rate","flag_n_dense","epochs","epochs_completed","metrics","model","loss_function","optimizer","learning_rate","script","start","end","completed","output","source_code","context","type"
"5","runs/2022-05-24T13-44-56Z",0.8556,0.7452,0.7883,0.5814,1e-04,0.2,1024,6,6,"runs/2022-05-24T13-44-56Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 1024)                    2098176     Y          
 activation (Activation)            (None, 1024)                    0           Y          
 dropout (Dropout)                  (None, 1024)                    0           Y          
 dense (Dense)                      (None, 50)                      51250       Y          
===========================================================================================
Total params: 23,010,906
Trainable params: 2,149,426
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f016818bd90>",9.99999974737875e-05,"finetuning.R",2022-05-24 13:44:56,2022-05-24 14:15:08,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  1e-04
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  1024","runs/2022-05-24T13-44-56Z/tfruns.d/source.tar.gz","local","training"
"7","runs/2022-05-24T12-31-03Z",0.8417,0.7662,0.7839,0.5984,1e-04,0.3,1024,6,6,"runs/2022-05-24T12-31-03Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 1024)                    2098176     Y          
 activation (Activation)            (None, 1024)                    0           Y          
 dropout (Dropout)                  (None, 1024)                    0           Y          
 dense (Dense)                      (None, 50)                      51250       Y          
===========================================================================================
Total params: 23,010,906
Trainable params: 2,149,426
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f01b817d580>",9.99999974737875e-05,"finetuning.R",2022-05-24 12:31:03,2022-05-24 13:09:06,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  1e-04
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  1024","runs/2022-05-24T12-31-03Z/tfruns.d/source.tar.gz","local","training"
"8","runs/2022-05-24T11-47-30Z",0.8194,0.7335,0.768,0.5847,0.001,0.3,1024,6,6,"runs/2022-05-24T11-47-30Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 1024)                    2098176     Y          
 activation (Activation)            (None, 1024)                    0           Y          
 dropout (Dropout)                  (None, 1024)                    0           Y          
 dense (Dense)                      (None, 50)                      51250       Y          
===========================================================================================
Total params: 23,010,906
Trainable params: 2,149,426
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f027a36b190>",0.00100000004749745,"finetuning.R",2022-05-24 11:47:47,2022-05-24 12:31:03,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  0.001
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  1024","runs/2022-05-24T11-47-30Z/tfruns.d/source.tar.gz","local","training"
"1","runs/2022-05-24T15-45-31Z",0.816,1.1594,0.7092,0.9123,1e-04,0.2,256,6,6,"runs/2022-05-24T15-45-31Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 256)                     524544      Y          
 activation (Activation)            (None, 256)                     0           Y          
 dropout (Dropout)                  (None, 256)                     0           Y          
 dense (Dense)                      (None, 50)                      12850       Y          
===========================================================================================
Total params: 21,398,874
Trainable params: 537,394
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f015927b9d0>",9.99999974737875e-05,"finetuning.R",2022-05-24 15:45:31,2022-05-24 16:15:17,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  1e-04
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  256","runs/2022-05-24T15-45-31Z/tfruns.d/source.tar.gz","local","training"
"6","runs/2022-05-24T13-09-06Z",0.8083,0.7663,0.7644,0.5982,0.001,0.2,1024,6,6,"runs/2022-05-24T13-09-06Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 1024)                    2098176     Y          
 activation (Activation)            (None, 1024)                    0           Y          
 dropout (Dropout)                  (None, 1024)                    0           Y          
 dense (Dense)                      (None, 50)                      51250       Y          
===========================================================================================
Total params: 23,010,906
Trainable params: 2,149,426
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f01b8177d90>",0.00100000004749745,"finetuning.R",2022-05-24 13:09:07,2022-05-24 13:44:56,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  0.001
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  1024","runs/2022-05-24T13-09-06Z/tfruns.d/source.tar.gz","local","training"
"4","runs/2022-05-24T14-15-08Z",0.7944,0.9642,0.7075,0.6909,0.001,0.3,256,6,6,"runs/2022-05-24T14-15-08Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 256)                     524544      Y          
 activation (Activation)            (None, 256)                     0           Y          
 dropout (Dropout)                  (None, 256)                     0           Y          
 dense (Dense)                      (None, 50)                      12850       Y          
===========================================================================================
Total params: 21,398,874
Trainable params: 537,394
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f015b2f8190>",0.00100000004749745,"finetuning.R",2022-05-24 14:15:08,2022-05-24 14:44:22,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  0.001
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  256","runs/2022-05-24T14-15-08Z/tfruns.d/source.tar.gz","local","training"
"2","runs/2022-05-24T15-14-28Z",0.7889,0.9532,0.7063,0.7176,0.001,0.2,256,6,6,"runs/2022-05-24T15-14-28Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 256)                     524544      Y          
 activation (Activation)            (None, 256)                     0           Y          
 dropout (Dropout)                  (None, 256)                     0           Y          
 dense (Dense)                      (None, 50)                      12850       Y          
===========================================================================================
Total params: 21,398,874
Trainable params: 537,394
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f0159d4a1f0>",0.00100000004749745,"finetuning.R",2022-05-24 15:14:28,2022-05-24 15:45:31,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  0.001
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  256","runs/2022-05-24T15-14-28Z/tfruns.d/source.tar.gz","local","training"
"3","runs/2022-05-24T14-44-22Z",0.7847,1.2182,0.6835,0.9373,1e-04,0.3,256,6,6,"runs/2022-05-24T14-44-22Z/tfruns.d/metrics.json","Model: ""sequential""
___________________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     Trainable  
===========================================================================================
 xception (Functional)              (None, 7, 7, 2048)              20861480    N          
 global_average_pooling2d (GlobalAv  (None, 2048)                   0           Y          
 eragePooling2D)                                                                           
 dense_1 (Dense)                    (None, 256)                     524544      Y          
 activation (Activation)            (None, 256)                     0           Y          
 dropout (Dropout)                  (None, 256)                     0           Y          
 dense (Dense)                      (None, 50)                      12850       Y          
===========================================================================================
Total params: 21,398,874
Trainable params: 537,394
Non-trainable params: 20,861,480
___________________________________________________________________________________________","categorical_crossentropy","<keras.optimizers.optimizer_v2.adam.Adam object at 0x7f015a701820>",9.99999974737875e-05,"finetuning.R",2022-05-24 14:44:23,2022-05-24 15:14:28,TRUE,"
> FLAGS <- flags(flag_numeric(""learning_rate"", 0.001), 
+     flag_numeric(""dropout_rate"", 0.2), flag_integer(""n_dense"", 
+         1024))

> global.seed <- 2022

> set.seed(global.seed)

> path.train <- ""train/""

> path.test <- ""test/""

> label.list <- dir(path.train)

> output.n <- length(label.list)

> width <- height <- 224

> target.size <- c(width, height)

> rgb <- 3

> batch_size <- 32

> epochs <- 6

> train.data.gen <- image_data_generator(rescale = 1/255, 
+     validation_split = 0.2)

> train.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""training"", target_size = target.size, 
+     class_mode = ""c ..."" ... [TRUNCATED] 

> validation.images <- flow_images_from_directory(path.train, 
+     train.data.gen, subset = ""validation"", target_size = target.size, 
+     class_mo .... [TRUNCATED] 

> test.data.gen <- image_data_generator(rescale = 1/255)

> test.images <- flow_images_from_directory(path.test, 
+     test.data.gen, target_size = target.size, class_mode = ""categorical"", 
+     classes = l .... [TRUNCATED] 

> mod.base <- application_xception(weights = ""imagenet"", 
+     include_top = FALSE, input_shape = c(width, height, 3))

> freeze_weights(mod.base)

> model.function <- function(learning_rate = 0.001, 
+     dropoutrate = 0.2, n_dense = 1024) {
+     k_clear_session()
+     model <- keras_model_seq .... [TRUNCATED] 

> model <- model.function(learning_rate = FLAGS$learning_rate, 
+     dropoutrate = FLAGS$dropout_rate, n_dense = FLAGS$n_dense)

> hist <- model %>% fit(train.images, steps_per_epoch = train.images$n%/%batch_size, 
+     epochs = epochs, validation_data = validation.images, vali .... [TRUNCATED] 

> cat(""The learning rate was "", FLAGS$learning_rate)
The learning rate was  1e-04
> cat(""The dropout rate was "", FLAGS$dropput_rate)
The dropout rate was 
> cat(""The n_dense was "", FLAGS$n_dense)
The n_dense was  256","runs/2022-05-24T14-44-22Z/tfruns.d/source.tar.gz","local","training"
