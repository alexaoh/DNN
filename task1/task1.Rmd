---
title: "Task1"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Liv Breivik, Hannes Johansson, Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: show
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>")
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

set.seed(111)
```

The objective of this task is to use information on protein abundance and gene expression of patients to predict the breast invasive carcinoma (BRCA) estrogen receptor status. 

# Protein Abundance and Gene Expression Datasets


```{r, cache = T}
gene.exp <- read_delim("gene_expression.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
prot.ab <- read_delim("protein_abundance.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
clinical <- read_delim("clinical.csv","\t", escape_double = FALSE, trim_ws = TRUE)
```

The dimensions of the protein abundance dataset are shown below. 

```{r}
dim(prot.ab)
```

```{r}
all(complete.cases(prot.ab)) # TRUE: There are no missing values. 
```

The protein abundance dataset has no missing values. It contains 410 unique patients, where each patient has its own sampling code and a numerical value that gives the abundance of 142 different proteins. 

The dimensions of the gene expression dataset are shown below. 

```{r}
dim(gene.exp)
```

```{r}
all(complete.cases(gene.exp)) # FALSE: There are missing values in some of the columns. 
```

The gene expression dataset has missing values. The missing values are simply removed from the dataset in the following block of code. 

```{r}
dim(gene.exp)
gene.exp2 <- gene.exp[complete.cases(gene.exp),]
dim(gene.exp2)
```

Before the rows with missing values are removed, the gene expression data contains `r dim(gene.exp)[[1]]` unique patients, each of which has its own sampling code. After the rows with missing data are removed, the gene expression data contains `r dim(gene.exp2)[[1]]` unique patients. 

Next we find which patients have data of both types available, keeping in mind that the response we want to predict is contained in the `clinical` data. Before continuing, note that the `clinical` data has the following dimensions

```{r}
dim(clinical)
```

and we only care about columns 1 and 9. Column 1 contains the identifier of each patient, which are `r dim(clinical)[[1]]` in total in this dataset. Column 9 contains the response we want to predict, which has the following unique values

```{r}
unique(clinical[,9])
```

These values will be preprocessed later. Below the code used to find patients that have data available is given. 

```{r}
full.gene.clin <- intersect(gene.exp2$Sample, clinical$Sample)
length(full.gene.clin)
```

The first intersection that is shown is the intersection between the gene expression data with no missing values and the clinical data, i.e. this intersection now contains all unique patient identifiers that exist in the gene data and have recorded the response. Notice that this the data that will be used in the first part of the task. 

```{r}
int.gene.prot <- intersect(gene.exp$Sample, prot.ab$Sample) 
length(int.gene.prot)
```

The second intersection that is shown is the intersection between the gene expression data which still contains missing values and the protein abundance data. This will not be used in the analysis, but is given because it might be interesting to keep in mind. Thus we can see that `r length(int.gene.prot)` of the patients' sample codes exist in both the protein abundance and the gene expression datasets, before removing the rows with missing values from the gene expression data. 

```{r}
int.gene.full.prot <- intersect(gene.exp2$Sample, prot.ab$Sample)
length(int.gene.full.prot)
```

The intersection above gives the unique patients that have no missing gene expression data and have recorded protein abundance data. This is used in order to define the next intersection. 

```{r}
full.gene.prot.clin <- intersect(int.gene.full.prot, clinical$Sample) 
length(full.gene.prot.clin)
```

The last intersection gives the intersection between the third list of patients (`int.gene.full.prot`) and the patients in the `clinical` dataset. Thus, this contains the unique list of patients that have all necessary data in all three datasets. Notice that these patients will be used to define the complete dataset, which will be used for the concatenated model later in the analysis. 

As noted earlier, we will now only use the intersection `full.gene.clin`. Even though we know that all these patients have a recorded breast invasive carcinoma (BRCA) estrogen receptor status, we need to check that the values they have recorded are either `Positive` or `Negative`, keeping in mind the values we saw that the `clinical` dataset contains. Next, we remove all the individuals that don't have this information. 

```{r}
chosen.data <-  full.gene.clin

xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
#xprot <- prot.ab[prot.ab$Sample%in%chosen.data,]
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate" or "Not Performed".
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
```

## Gene Expression Data

From now on we only use the **gene expression data**. That's why we in the questions 2-6 will use the last mentioned set above which now consists of the patients that has both complete gene expression dataset and a data concerning their breast invasive carcinoma (BRCA) estrogen receptor status: positive or negative. (Later in question 7 and 8, the patients with complete data will be used, that is only using the set of patients that is also included in the protein abundance dataset, as mentioned earlier.)

### Select the 25% of genes with the most variability.

The 25% percent of genes with the most variability are chosen. Information about the genes chosen are stored and reused later in the selection of the genes for the other set in section 4 onwards, to make sure that the same set of genes that the network was trained with are the ones selected for that set as well. 

```{r}
percentage <- round(dim(mgene[,-c(1,2)])[[2]]*0.25) # Find how many variables correspond to 25%. 
variances <- apply(X=mgene[,-c(1,2)], MARGIN=2, FUN=var) # Find empirical variance in each of the variables (genes).
sorted <- sort(variances, decreasing=TRUE, index.return=TRUE)$ix[1:percentage] # Sort from highest to lowest variance and select the top 25% indices. 
mgene.lvar <- mgene[, c(1,2,sorted)] # Select the 25% largest variance variables using the indices found above. 
```

The selected `r percentage` genes are used to implement a stacked autoencoder (SAE) with three stacked layers of 1000, 100 and 50 nodes. 

## Final Training/Test Split

```{r}
set.seed(111)
training.fraction <- 0.70 # 75 % of data will be used for training. 
training <- sample(1:nrow(mgene.lvar),nrow(mgene.lvar)*training.fraction) 

xtrain <- mgene.lvar[training,-c(1,2)]
xtest <- mgene.lvar[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 
xtrain <- scale(data.matrix(xtrain)) 
xtest <- scale(data.matrix(xtest))

# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0

# The data is saved to a file, so that it can be loaded directly into tfruns() files. 
data.train <- data.frame(ylabels, xtrain)
data.test <- data.frame(ytestlabels, xtest)
write.csv(data.train, "train_for5.csv")
write.csv(data.test, "test_for5.csv")
```

# Implementation of SAE

In this section a stacked autoencoder (SAE) will be implemented. It will consist of three stacked layers of 1000, 100 and 50 nodes. In each case, some qualitative evidence of the quality of coding obtained will be given, in the form of correlation plots between input and output. 

## First Layer (1000 nodes)
```{r}
# Develop the encoder. 
input_enc1 <- layer_input(shape = percentage)
output_enc1 <- input_enc1 %>% 
  layer_dense(units=1000,activation="relu") 
encoder1 <- keras_model(input_enc1, output_enc1)
summary(encoder1)

# Develop the decoder. 
input_dec1 <- layer_input(shape = 1000)
output_dec1 <- input_dec1 %>% 
  layer_dense(units = percentage, activation="linear")
decoder1 <- keras_model(input_dec1, output_dec1)
summary(decoder1)

# Develop the first AE.
aen_input1 <- layer_input(shape = percentage)
aen_output1 <- aen_input1 %>% 
  encoder1() %>% 
  decoder1()
sae1 <- keras_model(aen_input1, aen_output1)
summary(sae1)
```

We compile the model and fit it to the training data. To decide the final number of epochs each 'val-loss'-value is regarded after each epoch. If the value has not decreased in a certain number of epochs the training will stop. This is to reduce the risk of overfitting the network, that is that the network may otherwise learn patterns that are too specific for the training data while the performance on the actual validation data begins to decrease. 

```{r comp}
sae1 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r fit}
callbacks_parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 12,
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

sae1 %>% fit(
  x = xtrain,
  y = xtrain,
  epochs = 40,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data. 

```{r}
encoded_expression1 <- encoder1 %>% predict(xtrain) 
decoded_expression1 <- decoder1 %>% predict(encoded_expression1)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae1,xtrain)
```

Some (weak) evidence of the quality of the coding obtained follows. We plot the correlation between the predictions from the autoencoder and the correct data, both on the train and test sets. The results are shown below. 

```{r}
vcor <- diag(cor(x.hat,xtrain))
hist(vcor, main = "Correlation on Training Data")
```

The histogram above shows that the correlation between the training data and the predictions from the first autoencoder are relatively high. Worth menitioning is that if we had not implemented the training stop when no progression of the validation data was registered, the correlation on the training set would be significantly higher, as the network would continue to fit the parameters to the training data. As stated, this would probably have resulted in worse values for the initial test set that we set aside before the training started. We do the same check on the test set. 

```{r}
x.hat <- predict(sae1,xtest)
vcor <- diag(cor(x.hat,xtest))
hist(vcor, main = "Correlation on Testing Data")
```

As expected, the correlation is lower on the test data, but there still is some correlation. 

## Second Layer (100 nodes)
```{r}
# Develop the encoder. 
input_enc2 <- layer_input(shape = 1000)
output_enc2 <- input_enc2 %>% 
  layer_dense(units=100,activation="relu") 
encoder2 <- keras_model(input_enc2, output_enc2)
summary(encoder2)

# Develop the decoder. 
input_dec2 <- layer_input(shape = 100)
output_dec2 <- input_dec2 %>% 
  layer_dense(units = 1000, activation="linear")
decoder2 <- keras_model(input_dec2, output_dec2)
summary(decoder2)

# Develop the second AE.
aen_input2 <- layer_input(shape = 1000)
aen_output2 <- aen_input2 %>% 
  encoder2() %>% 
  decoder2()
sae2 <- keras_model(aen_input2, aen_output2)
summary(sae2)
```

We compile the model and fit it to the training data. 

```{r}
sae2 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
callbacks_parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 8,
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

sae2 %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 70,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder. Also here we use the automatic training stop when the performance on the validation set has stopped improving.

```{r}
encoded_expression2 <- encoder2 %>% predict(encoded_expression1) 
decoded_expression2 <- decoder2 %>% predict(encoded_expression2)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae2,encoded_expression1)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression1))
hist(vcor, main = "Correlation with Input Data (Encoded from sae1)")
```

The histogram above shows that there is correlation between the input and the predictions from the second autoencoder. Note that we do not have a test set in this case, since the dimension of the output data from sae2 is 1000, which is less than the amount of features in the test data. 

## Third Layer (50 nodes)
```{r}
# Develop the encoder. 
input_enc3 <- layer_input(shape = 100)
output_enc3 <- input_enc3 %>% 
  layer_dense(units=50,activation="relu") 
encoder3 <- keras_model(input_enc3, output_enc3)
summary(encoder3)

# Develop the decoder. 
input_dec3 <- layer_input(shape = 50)
output_dec3 <- input_dec3 %>% 
  layer_dense(units = 100, activation="linear")
decoder3 <- keras_model(input_dec3, output_dec3)
summary(decoder3)

# Develop the third AE.
aen_input3 <- layer_input(shape = 100)
aen_output3 <- aen_input3 %>% 
  encoder3() %>% 
  decoder3()
sae3 <- keras_model(aen_input3, aen_output3)
summary(sae3)
```

We compile the model and fit it to the training data. 

```{r}
sae3 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
sae3 %>% fit(
  x = encoded_expression2,
  y = encoded_expression2,
  epochs = 180,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder.  

```{r}
encoded_expression3 <- encoder3 %>% predict(encoded_expression2) 
decoded_expression3 <- decoder3 %>% predict(encoded_expression3)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae3,encoded_expression2)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression2))
hist(vcor, main = "Correlation with Input Data (Encoded from sae2)")
```

The histogram above shows that there is some correlation between the input and the predictions from the second autoencoder. Notice that the correlation is less than earlier though. Also note that we do not have a test set in this case, since the dimension of the output data from sae3 is 100, which is less than the amount of features in the test data. 

TODO: DUBBELKOLLA VARFÖR TRÄNINGEN KÖR SÅ HIMLA LÄNGE I DETTA FALLET. HAR LETAT: DET KAN VARA KORREKT MEN DET KAN OCKSÅ HA SKETT PÅ GRUND AV NÅGOT TIDIGARE FEL NÅGONSTANS NÄR VI FÖRST SKREV DELEN.

## Final Model (SAE)

The final stacked autoencoder (SAE) is constructed below. All the encoders are stacked previously trained are stacked together. 

```{r}
sae_input <- layer_input(shape = percentage, name = "gene.mod")
sae_output <- sae_input %>% 
  encoder1() %>% 
  encoder2() %>%
  encoder3()
   
sae <- keras_model(sae_input, sae_output)
summary(sae)

# Code below is used for loading model (and model checking) in separate file for tfruns().
yhat <- predict(sae, xtest)
write.csv(yhat, file = "predictions.csv")
save_model_weights_hdf5(sae, "sae.hdf5")
```

# SAE as Pre-Training Model for Prediction of Estrogen Receptor State

The SAE is used as a pre-training model for prediction of the estrogen receptor state. The DNN has 10 nodes in the first layer, followed by one output node. The weights are frozen for the first 3 functional layers, which means that only the weights from the third autoencoder to the first fully connected layer and from the first layer in the DNN to the output layer (in total 521 weights) are to be fine-tuned to obtain the final classifier. 

```{r}
sae_output2 <- sae_output %>%
  layer_dense(10,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)
```

We compile and fit the final classifier. 

```{r}
sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)
```

  
```{r}
sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 80,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

## Performance Metrics

The model is evaluated on the test set below.  

```{r}
sae %>%
  evaluate(xtest, ytestlabels)
```

Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below. 

```{r}
yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))
```

The ROC curve is shown below. 

```{r}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

As is seen above, the AUC is `r round(roc_sae_test$auc, 2)`. COMMENTS / INTERPRETATIONS!

For an AUC curve, a score of 1 would signify a perfect predictor whilst a score of 0.5 would signify that the predictor is just as good as a random generator (i.e. equivalent to flipping a coin). A score of `r round(roc_sae_test$auc, 2)` is therefore considered acceptable as it should be able to distinguish the various cases to some extent, however there is big room for improvement.

# Use `tfruns` to Explore Configurations of First Fully Connected Layer

In this section, `tfruns` is used to explore what amount of nodes in the first layer of the DNN gives the best performance. We are asked to search among three different numbers; 5, 10 and 20. The code used with `tfruns` is given in separate R files. Anyhow, the exploration leads to the conclusion that the configuration with 20 nodes gives the best results. 

Thus, the final model is

```{r}
sae_output2 <- sae_output %>%
  layer_dense(20,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)

sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)

sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 80,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)

sae %>%
  evaluate(xtest, ytestlabels)

yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))

roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```
From the ROC curve we see that the AUC-score now is closer to 1, indicating that this is a better model.


DENNE ER JO DÅRLIGERE. HVORDAN ORDNE OPP IT STOKASTISITET I TFRUNS + HER? HVILKEN VARIANT SOM ER BEST (5, 10 ELLER 20 NODER) VIRKER VELDIG TILFELDIG (OG AVHENGER OGSÅ STERKT AV HVOR MANGE VEKTER SOM FRYSES). I TILLEGG VIRKER DET IKKE SOM AT metric_val_acc ER ET GODT MÅL PÅ HVILKEN MODELL SOM GIR BEST RESULTATER, DA AUC OG ROC I FLERE TILFELLER VISER NOE ANNET. 

??? Liv her, AUC nærmere 1 betyr jo at den er bedre? ... må huske å seede kanskje btw

## Complete Data 

The **complete data** (gene expression and protein abundance) is split into train and test sets. 

ATT LÄGGA IN VID DEN MOTSVARANDE KODEN: As explained in section 1, the same set of gene features is used in this dataset, in order to secure that the input now consists of the same genes that the network was trained on in previous sections. 

```{r}
chosen.data <- full.gene.prot.clin # Change this definition after deciding which data set to use!
xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
xprot <- prot.ab[prot.ab$Sample%in%chosen.data,]
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate" or "Not Performed".
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
mtot <- merge(mgene, xprot, by.x = "Sample", by.y = "Sample")
#mtot <- xclin %>% left_join(xgene) %>% left_join(xprot, by = "Sample") # This gives the same result. 

set.seed(111)
training.fraction <- 0.70 # 75 % of data will be used for training. 
training <- sample(1:nrow(mtot),nrow(mtot)*training.fraction) 

xtrain <- mtot[training,-c(1,2)]
xtest <- mtot[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 
xtrain <- scale(data.matrix(xtrain)) 
xtest <- scale(data.matrix(xtest))

# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0
```

```{r}
xprot_train <- xprot[training,-c(1,2)]
xgene_train <- xgene[training,-c(1,2)]
xprot_test <- xprot[-training,-c(1,2)]
xgene_test <- xgene[-training,-c(1,2)]

ytrain_bin <- to_categorical(as.array(ylabels), 2)
```

The SAE for the abundance of proteins (from class examples) is added in the code block below. Then, this model is concatenated with the former model, that was trained on the gene expression data. 

## Importing the SAE from the class example.

How I understood it: in the file aes_practice_3.R there is a SAE model that we can use for the breast stuff. We need to import this model and concatenate it with the one we created for gene expression.

```{r}
# Model given in class. 
source("aes_practice_3.R") # source: Parses the code and evaluates it in this environment. 
```


# Concatenated Model

In this section we concatenate the two SAEs to fit, on the trainset, a DNN that integrates both data sources to predict estrogen receptor status. The DNN has a dense layer, with 20 nodes, which where the best amount of nodes found with `tfruns` earlier, in addition to the output layer. 

```{r}
concatenated<-layer_concatenate(list(sae_output2,sae_protab_output))
model_output<-concatenated %>% 
  layer_dense(20,"relu") %>% 
  layer_dense(units=1,activation="softmax")

model<-keras_model(list(sae_input,sae_protab_input), model_output)
summary(model)
```


```{r, eval = F}
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = "acc"
)

# training 
#Den her funker ikke. Jeg tror det er fordi xgene_train har 17000 ish kolonner. Den burde egentlig ha typ 4454 eller noe liknende. Må se litt på datarensing. (kan også hende jeg har gjort ting helt feil i tillegg)
model %>% fit(
  list(gene.mod=xgene_train, prot.mod=xprot_train), as.array(ylabels),
  epochs = 75, batch_size = 64, validation_split = 0.2
)

```


KODEN NEDENFOR FUNGERER IKKE AKKURAT NÅ, USIKKER PÅ HVORFOR DEN FAILER. 

Liv her: jeg tror den failer pga denne SAE skal kun brukes på protein abundance greia. Egt ville det beste vært om vi kunne eksportert modellen fra scriptet til læreren, siden det er nøyaktig det vi skal bruke. Hvis ikke burde vi ikke definere xtrain som noe nytt.


^Gjort ovenfor. Koden nedenfor kan slettes, men jeg har ikke lyst til å slette andre sitt arbeid. (Vi sitter nå med modellene sae og sae_protab)

```{r, eval = F}
## ---------------------------------------------------------------------------------
input_enc1<-layer_input(shape = c(142))
output_enc1<-input_enc1 %>% 
  layer_dense(units=50,activation="relu") 
encoder1 = keras_model(input_enc1, output_enc1)
summary(encoder1)


## ---------------------------------------------------------------------------------
input_dec1 = layer_input(shape = c(50))
output_dec1<-input_dec1 %>% 
  layer_dense(units=142,activation="linear")

decoder1 = keras_model(input_dec1, output_dec1)
 
summary(decoder1)


## ---------------------------------------------------------------------------------
aen_input1 = layer_input(shape = c(142))
aen_output1 = aen_input1 %>% 
  encoder1() %>% 
  decoder1()
   
sae1 = keras_model(aen_input1, aen_output1)
summary(sae1)


## ---------------------------------------------------------------------------------
sae1 %>% compile(
  loss = "mse",
  optimizer = "rmsprop",
  metrics = c('accuracy')
)


## ---------------------------------------------------------------------------------
sae1 %>% fit(
  x=as.matrix(xtrain),
  y=ylabels,
  epochs = 100,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
  )
#y=as.matrix(xtrain)


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression1 <- encoder1 %>% predict(as.matrix(xtrain))


## ---------------------------------------------------------------------------------
input_enc2<-layer_input(shape = c(50))
output_enc2<-input_enc2 %>% 
  layer_dense(units=20,activation="relu") 
encoder2 = keras_model(input_enc2, output_enc2)
summary(encoder2)


## ---------------------------------------------------------------------------------
input_dec2 = layer_input(shape = c(20))
output_dec2<-input_dec2 %>% 
  layer_dense(units=50,activation="linear")

decoder2 = keras_model(input_dec2, output_dec2)
 
summary(decoder2)


## ---------------------------------------------------------------------------------
aen_input2 = layer_input(shape = c(50))
aen_output2 = aen_input2 %>% 
  encoder2() %>% 
  decoder2()
   
sae2 = keras_model(aen_input2, aen_output2)
summary(sae2)


## ---------------------------------------------------------------------------------
sae2 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)


## ---------------------------------------------------------------------------------
sae2 %>% fit(
  x=as.matrix(encoded_expression1),
  y=as.matrix(encoded_expression1),
  epochs = 300,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
  )


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression2 <- encoder2 %>% predict(as.matrix(encoded_expression1))


## ---------------------------------------------------------------------------------
input_enc3<-layer_input(shape = c(20))
output_enc3<-input_enc3 %>% 
  layer_dense(units=10,activation="relu") 
encoder3 = keras_model(input_enc3, output_enc3)
summary(encoder3)


## ---------------------------------------------------------------------------------
input_dec3 = layer_input(shape = c(10))
output_dec3<-input_dec3 %>% 
  layer_dense(units=20,activation="linear")

decoder3 = keras_model(input_dec3, output_dec3)
 
summary(decoder3)


## ---------------------------------------------------------------------------------
aen_input3 = layer_input(shape = c(20))
aen_output3 = aen_input3 %>% 
  encoder3() %>% 
  decoder3()
   
sae3 = keras_model(aen_input3, aen_output3)
summary(sae3)


## ---------------------------------------------------------------------------------
sae3 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)


## ---------------------------------------------------------------------------------
sae3 %>% fit(
  x=as.matrix(encoded_expression2),
  y=as.matrix(encoded_expression2),
  epochs = 300,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
  )


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression3 <- encoder3 %>% predict(as.matrix(encoded_expression2))

sae_input.prot = layer_input(shape = c(142), name = "prot.mod")
sae_output.prot = sae_input %>% 
  encoder1() %>% 
  encoder2()  %>%
  encoder3() %>%
  layer_dense(20,activation = "relu")%>%
  layer_dense(1,activation = "sigmoid")
```

The two models are concatenated below. CHECK THAT THIS IS DONE CORRECTLY LATER!
SKAL DENSE LAYERS LEGGES TIL I DEN KONKATENERTE MODELLEN ELLER I HVER AV DE TO MODELLENE FØR DE KONKATINERES (SLIK DET ER GJORT NÅ)?




```{r, eval = F}
concatenated<-layer_concatenate(list(sae_output2,sae_output.prot))
model_output<-concatenated %>% 
  layer_dense(units=2,activation = "softmax")

model<-keras_model(list(sae_input,sae_output.prot), model_output)
summary(model)
```

The model is compiled and trained below. 

```{r, eval = F}
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = "acc"
)


# training 

model %>% fit( # Usikker på om det er rett at begge skal ha den samme dataen!?
  # Mulig dette blir helt feil dimensjoner!
  list(gene.mod=xtrain,prot.mod=xtrain), ylabels,
  epochs = 300, batch_size = 16, validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

## Performance Metrics

The model is evaluated on the test set below.  

```{r, eval = F}
model %>%
  evaluate(xtest, ytestlabels)
```

<!-- Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below.  -->

```{r, eval = F}
yhat <- predict(model,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))
```

The ROC curve is shown below. 

```{r, eval = F}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

# Discussion
