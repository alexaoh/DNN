---
title: "Task1"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: show
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>")
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)
```

# Protein Abundance and Gene Expression Datasets


```{r, cache = T}
gene.exp <- read_delim("gene_expression.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
prot.ab <- read_delim("protein_abundance.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
clinical <- read_delim("clinical.csv","\t", escape_double = FALSE, trim_ws = TRUE)
copy_number <- read_delim("copy_number.csv","\t", escape_double = FALSE, trim_ws = TRUE)
```
### 1. Describe protein abundance and gene expression datasets. How many patients have data of both types available. Are there missing data from some of the datasets? Preprocess them if necessary

The dimensions of the protein abundance dataset are shown below. 

```{r}
dim(prot.ab)
```

```{r}
all(complete.cases(prot.ab)) # TRUE: There are no missing values. 
```

The dataset has no missing values. It contains 410 unique patients, each of which has its own sampling code, where each patient has recorded a numerical value that gives abundance of 142 different proteins. 

The dimensions of the gene expression dataset are shown below. 

```{r}
dim(gene.exp)
```

```{r}
all(complete.cases(gene.exp)) # FALSE: There are missing values in some of the columns. 
```

The dataset has missing values. The missing values are removed from the dataset in the following block of code. 

```{r}
dim(gene.exp)
gene.exp2 <- gene.exp[complete.cases(gene.exp),]
dim(gene.exp2)
```

Before the rows with missing values are removed, the gene expression data contains `r dim(gene.exp)[[1]]` unique patients, each of which has its own sampling code. After the rows with missing data are removed, the gene expression data contains `r dim(gene.exp2)[[1]]` unique patients. 


```{r}
int1 <- intersect(gene.exp$Sample, prot.ab$Sample) 
length(int1)
int2 <- intersect(gene.exp2$Sample, prot.ab$Sample)
length(int2)
int3 <- intersect(int2, clinical$Sample) # Dette kan kanskje gjøres etter "massasjen" jeg gjør lenger nede!?
length(int3)

# Kanskje dataen nedenfor kan brukes i stedet? Dvs, vi bryr oss ikke om personer som har gene.exp data, men ikke har prot.ab data? Se hva de krever lenger nede!
int4 <- intersect(gene.exp2$Sample, clinical$Sample)
length(int4)

# Usikker på hva slags data vi skal bruke!

#Liv her. Jeg tolka det som at vi i punkt 2-6 benytter oss av gene expression data (med alle pasienter, t.o.m de som ikke har prot. ab data) og i 7-9 benytter oss av dataen til pasienter som har begge deler. På én side er det litt wack å ikke benytte seg av samme antall pasienter ift sammenligning (flere samples -> lettere å sammenlikne) men på en annen side ser vi jo på flere data når vi ser på begge datasettene, dvs blir input-data aldri likt uansett. Dette er up for discussion haha kan hende jeg har misforstått alt lol
```

From the code above we can see that `r length(int1)` of the patients' sample codes exist in the protein abundance and the gene expression datasets, before removing the rows with missing values from the gene expression data. This is not interesting to us however, since we are only interested in individuals who don't have missing gene expression data. After removing the missing values from the gene expression data, the number of patients that have data available for both gene expression and protein abundance is `r length(int2)`. Moreover, `r length(int3)` of the patients' sample codes exist in all three datasets, when the `clinical` dataset is included (without considering missing values in the `clinical` data. This is done next). This means that this amount of patients have data of both types available. But do all these individuals have data concerning their breast invasive carcinoma (BRCA) estrogen receptor status? Next, we remove all the individuals that don't have this information. 

```{r}
chosen.data <- int4 # Change this definition after deciding which data set to use!
xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
#xprot <- prot.ab[prot.ab$Sample%in%chosen.data,]
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate" or "Not Performed".
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
```

Note that the copy number data has no missing values, which means that we do not need to remove any data. 

## Gene Expression Data

From now on we only use the **gene expression data**. 

### 2. Select the 25% of genes with the most variability.

The 25% percent of genes with the most variability are chosen. 

```{r}
percentage <- round(dim(mgene[,-c(1,2)])[[2]]*0.25) # Find how many variables correspond to 25%. 
variances <- apply(X=mgene[,-c(1,2)], MARGIN=2, FUN=var) # Find empirical variance in each of the variables (genes).
sorted <- sort(variances, decreasing=TRUE, index.return=TRUE)$ix[1:percentage] # Sort from highest to lowest variance and select the top 25% indices. 
mgene.lvar <- mgene[, c(1,2,sorted)] # Select the 25% largest variance variables using the indices found above. 
```

The selected `r percentage` genes are used to implement a stacked autoencoder (SAE) with three stacked layers of 1000, 100 and 50 nodes. 

## Final Training/Test Split

```{r}
set.seed(111)
training.fraction <- 0.7 # 70 % of data will be used for training. 
training <- sample(1:nrow(mgene.lvar),nrow(mgene.lvar)*training.fraction) 

xtrain <- mgene.lvar[training,-c(1,2)]
xtest <- mgene.lvar[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 
xtrain <- scale(data.matrix(xtrain)) 
xtest <- scale(data.matrix(xtest))

# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0

# The data is saved to a file, so that it can be loaded directly into tfruns() files. 
data.train <- data.frame(ylabels, xtrain)
data.test <- data.frame(ytestlabels, xtest)
write.csv(data.train, "train_for5.csv")
write.csv(data.test, "test_for5.csv")
```



### 3. Implement an stacked autoencoder (SAE) with three stacked layers of 1000, 100, 50 nodes. Provide in each case evidence of the quality of the coding obtained.


# Implementation of SAE

## First Layer (1000 nodes)
```{r}
# Develope the encoder. 
input_enc1 <- layer_input(shape = percentage)
output_enc1 <- input_enc1 %>% 
  layer_dense(units=1000,activation="relu") 
encoder1 <- keras_model(input_enc1, output_enc1)
summary(encoder1)

# Develope the decoder. 
input_dec1 <- layer_input(shape = 1000)
output_dec1 <- input_dec1 %>% 
  layer_dense(units = percentage, activation="linear")
decoder1 <- keras_model(input_dec1, output_dec1)
summary(decoder1)

# Develop the first AE.
aen_input1 <- layer_input(shape = percentage)
aen_output1 <- aen_input1 %>% 
  encoder1() %>% 
  decoder1()
sae1 <- keras_model(aen_input1, aen_output1)
summary(sae1)
```

We compile the model and fit it to the training data. 

```{r comp}
sae1 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r fit}
sae1 %>% fit(
  x = xtrain,
  y = xtrain,
  epochs = 25,
  batch_size = 64,
  validation_split = 0.2
)
```

We make predictions on the training data. 

```{r}
encoded_expression1 <- encoder1 %>% predict(xtrain) 
decoded_expression1 <- decoder1 %>% predict(encoded_expression1)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae1,xtrain)
```

Some (weak) evidence of the quality of the coding obtained follows. We plot the correlation between the predictions from the autoencoder and the correct data, both on the train and test sets. The results are shown below. 

```{r}
vcor <- diag(cor(x.hat,xtrain))
hist(vcor, main = "Correlation on Training Data")
```

The histogram above shows that the correlation between the training data and the predictions from the first autoencoder are relatively high. We do the same check on the test set. 

```{r}
x.hat <- predict(sae1,xtest)
vcor <- diag(cor(x.hat,xtest))
hist(vcor, main = "Correlation on Testing Data")
```

As expected, the correlation is lower on the test data, but there still is some correlation. 

## Second Layer (100 nodes)
```{r}
# Develop the encoder. 
input_enc2 <- layer_input(shape = 1000)
output_enc2 <- input_enc2 %>% 
  layer_dense(units=100,activation="relu") 
encoder2 <- keras_model(input_enc2, output_enc2)
summary(encoder2)

# Develop the decoder. 
input_dec2 <- layer_input(shape = 100)
output_dec2 <- input_dec2 %>% 
  layer_dense(units = 1000, activation="linear")
decoder2 <- keras_model(input_dec2, output_dec2)
summary(decoder2)

# Develop the second AE.
aen_input2 <- layer_input(shape = 1000)
aen_output2 <- aen_input2 %>% 
  encoder2() %>% 
  decoder2()
sae2 <- keras_model(aen_input2, aen_output2)
summary(sae2)
```

We compile the model and fit it to the training data. 

```{r}
sae2 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
sae2 %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 25,
  batch_size = 64,
  validation_split = 0.2
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder.  

```{r}
encoded_expression2 <- encoder2 %>% predict(encoded_expression1) 
decoded_expression2 <- decoder2 %>% predict(encoded_expression2)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae2,encoded_expression1)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression1))
hist(vcor, main = "Correlation with Input Data (Encoded from sae1)")
```

The histogram above shows that there is correlation between the input and the predictions from the second autoencoder. Note that we do not have a test set in this case, since the dimension of the output data from sae2 is 1000, which is less than the amount of features in the test data. 

## Third Layer (50 nodes)
```{r}
# Develop the encoder. 
input_enc3 <- layer_input(shape = 100)
output_enc3 <- input_enc3 %>% 
  layer_dense(units=50,activation="relu") 
encoder3 <- keras_model(input_enc3, output_enc3)
summary(encoder3)

# Develop the decoder. 
input_dec3 <- layer_input(shape = 50)
output_dec3 <- input_dec3 %>% 
  layer_dense(units = 100, activation="linear")
decoder3 <- keras_model(input_dec3, output_dec3)
summary(decoder3)

# Develop the third AE.
aen_input3 <- layer_input(shape = 100)
aen_output3 <- aen_input3 %>% 
  encoder3() %>% 
  decoder3()
sae3 <- keras_model(aen_input3, aen_output3)
summary(sae3)
```

We compile the model and fit it to the training data. 

```{r}
sae3 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
sae3 %>% fit(
  x = encoded_expression2,
  y = encoded_expression2,
  epochs = 25,
  batch_size = 64,
  validation_split = 0.2
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder.  

```{r}
encoded_expression3 <- encoder3 %>% predict(encoded_expression2) 
decoded_expression3 <- decoder3 %>% predict(encoded_expression3)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae3,encoded_expression2)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression2))
hist(vcor, main = "Correlation with Input Data (Encoded from sae2)")
```

The histogram above shows that there is some correlation between the input and the predictions from the second autoencoder. Notice that the correlation is less than earlier though. Also note that we do not have a test set in this case, since the dimension of the output data from sae3 is 100, which is less than the amount of features in the test data. 

## Final Model (SAE)

The final stacked autoencoder (SAE) is constructed below. All the encoders are stacked previously trained are stacked together. 

```{r}
sae_input <- layer_input(shape = percentage, name = "gene.mod")
sae_output <- sae_input %>% 
  encoder1() %>% 
  encoder2() %>%
  encoder3()
   
sae <- keras_model(sae_input, sae_output)
summary(sae)

# Code below is used for loading model (and model checking) in separate file for tfruns().
yhat <- predict(sae, xtest)
write.csv(yhat, file = "predictions.csv")
save_model_weights_hdf5(sae, "sae.hdf5")
```

### 4. Using the SAE as pre-training model, couple it with a two-layer DNN to predict the state of the estrogen receptor. The DNN must have 10 nodes in the first layer followed by the output layer.

# SAE as Pre-Training Model for Prediction of Estrogen Receptor State

The SAE is used as a pre-training model for prediction of the estrogen receptor state. The DNN has 10 nodes in the first layer, followed by one output node. The weights are frozen for the first 3 functional layers, which means that only the weights from the third autoencoder to the first fully connected layer and from the first layer in the DNN to the output layer (in total 521 weights) are to be fine-tuned to obtain the final classifier. 

```{r}
sae_output2 <- sae_output %>%
  layer_dense(10,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)
```

We compile and fit the final classifier. 

```{r}
sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)
```

  
```{r}
sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 30,
  batch_size=64,
  validation_split = 0.2
)
```

# Performance Metrics

The model is evaluated on the test set below.  

```{r}
sae %>%
  evaluate(xtest, ytestlabels)
```

Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below. 

```{r}
yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))
```

The ROC curve is shown below. 

```{r}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

As is seen above, the AUC is `r round(roc_sae_test$auc, 2)`. COMMENTS / INTERPRETATIONS!

For an AUC curve, a score of 1 would signify a perfect predictor whilst a score of 0.5 would signify that the predictor is just as good as a random generator (i.e. equivalent to flipping a coin). A score of `r round(roc_sae_test$auc, 2)` is therefore considered acceptable as it should be able to distinguish the various cases to some extent, however there is big room for improvement.


### 6. With tfruns() repeat points 4 and 5, exploring the configurations of the first layer of the DNN based on 5, 10 and 20 nodes. Determine which configuration is the best.

# Use `tfruns()` to Explore Configurations of First Fully Connected Layer

The code used with `tfruns` is given in separate R files. Anyhow, the exploration of the first layer of the DNN containing 5, 10 or 20 nodes leads to the conclusion that the configuration with 20 nodes gives the best results. CHECK THIS AGAIN LATER!

Thus, the final model is

```{r}
sae_output2 <- sae_output %>%
  layer_dense(20,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)

sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)

sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 30,
  batch_size=64,
  validation_split = 0.2
)

sae %>%
  evaluate(xtest, ytestlabels)

yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))

roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```
From the ROC curve we see that the AUC-score now is closer to 1, indicating that this is a better model.


DENNE ER JO DÅRLIGERE. HVORDAN ORDNE OPP IT STOKASTISITET I TFRUNS + HER? HVILKEN VARIANT SOM ER BEST (5, 10 ELLER 20 NODER) VIRKER VELDIG TILFELDIG (OG AVHENGER OGSÅ STERKT AV HVOR MANGE VEKTER SOM FRYSES). I TILLEGG VIRKER DET IKKE SOM AT metric_val_acc ER ET GODT MÅL PÅ HVILKEN MODELL SOM GIR BEST RESULTATER, DA AUC OG ROC I FLERE TILFELLER VISER NOE ANNET. 

??? Liv her, AUC nærmere 1 betyr jo at den er bedre?

### 7. Split the set of patients with complete data (gene expression and protein abundance) in train and test sets.


## Complete Data 

The **complete data** (gene expression and protein abundance) is split into train and test sets. 

```{r}
chosen.data <- int3 # Change this definition after deciding which data set to use!
xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
xprot <- prot.ab[prot.ab$Sample%in%chosen.data,]
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate" or "Not Performed".
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
mtot <- merge(mgene, xprot, by.x = "Sample", by.y = "Sample")
#mtot <- xclin %>% left_join(xgene) %>% left_join(xprot, by = "Sample") # This gives the same result. 

set.seed(111)
training.fraction <- 0.7 # 70 % of data will be used for training. 
training <- sample(1:nrow(mtot),nrow(mtot)*training.fraction) 

xtrain <- mtot[training,-c(1,2)]
xtest <- mtot[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 
xtrain <- scale(data.matrix(xtrain)) 
xtest <- scale(data.matrix(xtest))

# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0
```

The SAE for the abundance of proteins (from class examples) is added in the code block below. Then, this model is concatenated with the former model, that was trained on the gene expression data. 

## Importing the SAE from the class example.

How I understood it: in the file aes_practice_3.R there is a SAE model that we can use for the breast stuff. We need to import this model and concatenate it with the one we created for gene expression.

```{r}



```


### 8. Concatenate the two SAEs to fit, on the trainset, a DNN that integrates both data sources to predict estrogen receptor status. The DNN must have a dense layer (with the better number of nodes according with point 6) and the output layer.

KODEN NEDENFOR FUNGERER IKKE AKKURAT NÅ, USIKKER PÅ HVORFOR DEN FAILER. 

Liv her: jeg tror den failer pga denne SAE skal kun brukes på protein abundance greia. Egt ville det beste vært om vi kunne eksportert modellen fra scriptet til læreren, siden det er nøyaktig det vi skal bruke. Hvis ikke burde vi ikke definere xtrain som noe nytt.

```{r, eval = F}
## ---------------------------------------------------------------------------------
input_enc1<-layer_input(shape = c(142))
output_enc1<-input_enc1 %>% 
  layer_dense(units=50,activation="relu") 
encoder1 = keras_model(input_enc1, output_enc1)
summary(encoder1)


## ---------------------------------------------------------------------------------
input_dec1 = layer_input(shape = c(50))
output_dec1<-input_dec1 %>% 
  layer_dense(units=142,activation="linear")

decoder1 = keras_model(input_dec1, output_dec1)
 
summary(decoder1)


## ---------------------------------------------------------------------------------
aen_input1 = layer_input(shape = c(142))
aen_output1 = aen_input1 %>% 
  encoder1() %>% 
  decoder1()
   
sae1 = keras_model(aen_input1, aen_output1)
summary(sae1)


## ---------------------------------------------------------------------------------
sae1 %>% compile(
  loss = "mse",
  optimizer = "rmsprop",
  metrics = c('accuracy')
)


## ---------------------------------------------------------------------------------
sae1 %>% fit(
  x=as.matrix(xtrain),
  y=ylabels,
  epochs = 25,
  batch_size=64,
  validation_split = 0.2
  )
#y=as.matrix(xtrain)


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression1 <- encoder1 %>% predict(as.matrix(xtrain))


## ---------------------------------------------------------------------------------
input_enc2<-layer_input(shape = c(50))
output_enc2<-input_enc2 %>% 
  layer_dense(units=20,activation="relu") 
encoder2 = keras_model(input_enc2, output_enc2)
summary(encoder2)


## ---------------------------------------------------------------------------------
input_dec2 = layer_input(shape = c(20))
output_dec2<-input_dec2 %>% 
  layer_dense(units=50,activation="linear")

decoder2 = keras_model(input_dec2, output_dec2)
 
summary(decoder2)


## ---------------------------------------------------------------------------------
aen_input2 = layer_input(shape = c(50))
aen_output2 = aen_input2 %>% 
  encoder2() %>% 
  decoder2()
   
sae2 = keras_model(aen_input2, aen_output2)
summary(sae2)


## ---------------------------------------------------------------------------------
sae2 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)


## ---------------------------------------------------------------------------------
sae2 %>% fit(
  x=as.matrix(encoded_expression1),
  y=as.matrix(encoded_expression1),
  epochs = 25,
  batch_size=64,
  validation_split = 0.2
  )


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression2 <- encoder2 %>% predict(as.matrix(encoded_expression1))


## ---------------------------------------------------------------------------------
input_enc3<-layer_input(shape = c(20))
output_enc3<-input_enc3 %>% 
  layer_dense(units=10,activation="relu") 
encoder3 = keras_model(input_enc3, output_enc3)
summary(encoder3)


## ---------------------------------------------------------------------------------
input_dec3 = layer_input(shape = c(10))
output_dec3<-input_dec3 %>% 
  layer_dense(units=20,activation="linear")

decoder3 = keras_model(input_dec3, output_dec3)
 
summary(decoder3)


## ---------------------------------------------------------------------------------
aen_input3 = layer_input(shape = c(20))
aen_output3 = aen_input3 %>% 
  encoder3() %>% 
  decoder3()
   
sae3 = keras_model(aen_input3, aen_output3)
summary(sae3)


## ---------------------------------------------------------------------------------
sae3 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)


## ---------------------------------------------------------------------------------
sae3 %>% fit(
  x=as.matrix(encoded_expression2),
  y=as.matrix(encoded_expression2),
  epochs = 25,
  batch_size=64,
  validation_split = 0.2
  )


## ---------------------------------------------------------------------------------
#Generating with Autoencoder
encoded_expression3 <- encoder3 %>% predict(as.matrix(encoded_expression2))

sae_input.prot = layer_input(shape = c(142), name = "prot.mod")
sae_output.prot = sae_input %>% 
  encoder1() %>% 
  encoder2()  %>%
  encoder3() %>%
  layer_dense(20,activation = "relu")%>%
  layer_dense(1,activation = "sigmoid")
```

The two models are concatenated below. CHECK THAT THIS IS DONE CORRECTLY LATER!
SKAL DENSE LAYERS LEGGES TIL I DEN KONKATENERTE MODELLEN ELLER I HVER AV DE TO MODELLENE FØR DE KONKATINERES (SLIK DET ER GJORT NÅ)?

```{r, eval = F}
concatenated<-layer_concatenate(list(sae_output2,sae_output.prot))
model_output<-concatenated %>% 
  layer_dense(units=2,activation = "softmax")

model<-keras_model(list(sae_input,sae_output.prot), model_output)
summary(model)
```

The model is compiled and trained below. 

```{r, eval = F}
model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = "acc"
)


# training 

model %>% fit( # Usikker på om det er rett at begge skal ha den samme dataen!?
  # Mulig dette blir helt feil dimensjoner!
  list(gene.mod=xtrain,prot.mod=xtrain), ylabels,
  epochs = 75, batch_size = 16, validation_split = 0.2
)
```

# Performance Metrics

The model is evaluated on the test set below.  

```{r, eval = F}
model %>%
  evaluate(xtest, ytestlabels)
```

<!-- Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below.  -->

```{r, eval = F}
yhat <- predict(model,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))
```

The ROC curve is shown below. 

```{r, eval = F}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

# Discussion
