---
title: "Task1"
subtitle: "Statistical Learning with Deep Artificial Neural Networks"
author: "Liv Breivik, Hannes Johansson, Alexander J Ohrt"
date: "`r format(Sys.time(), '%d. %b. %Y')`"
output: 
   html_document:
    code_folding: show
    theme: yeti
    highlight: textmate
    number_sections: true
    toc: true
    latex_engine: xelatex
params: 
  seed: 111
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>")
library(keras)
library(readr)
library(pROC)
library(tidyverse)
library(caret)
library(ggplot2)

set.seed(params$seed)
```

The objective of this task is to use information on protein abundance and gene expression of patients to predict the breast invasive carcinoma (BRCA) estrogen receptor status. 

# Protein Abundance and Gene Expression Datasets

```{r, cache = T}
gene.exp <- read_delim("gene_expression.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
prot.ab <- read_delim("protein_abundance.csv", "\t", escape_double = FALSE, trim_ws = TRUE)
clinical <- read_delim("clinical.csv","\t", escape_double = FALSE, trim_ws = TRUE)
```

The dimensions of the protein abundance dataset are shown below. 

```{r}
dim(prot.ab)
```

```{r}
all(complete.cases(prot.ab)) # TRUE: There are no missing values. 
```

The protein abundance dataset has no missing values. It contains 410 unique patients, where each patient has their own sampling code and a numerical value that gives the abundance of 142 different proteins. 

The dimensions of the gene expression dataset are shown below. 

```{r}
dim(gene.exp)
```

```{r}
all(complete.cases(gene.exp)) # FALSE: There are missing values in some of the columns. 
```

The gene expression dataset has missing values. The missing values are simply removed from the dataset in the following block of code. 

```{r}
dim(gene.exp)
gene.exp2 <- gene.exp[complete.cases(gene.exp),]
dim(gene.exp2)
```

Before the rows with missing values are removed, the gene expression data contains `r dim(gene.exp)[[1]]` unique patients, each of which has their own sampling code. After the rows with missing data are removed, the gene expression data contains `r dim(gene.exp2)[[1]]` unique patients. 

Next we find the patients with data of both types available, keeping in mind that the response we want to predict is contained in the `clinical` data. Before continuing, note that the `clinical` data has the following dimensions

```{r}
dim(clinical)
```

and we only care about columns 1 and 9. Column 1 contains the identifier of each patient, which are `r dim(clinical)[[1]]` in total in this dataset. Column 9 contains the response we want to predict, which has the following unique values

```{r}
unique(clinical[,9])
```

These values will be pre-processed later. Below the code used to find patients that have data available is given. 

```{r}
full.gene.clin <- intersect(gene.exp2$Sample, clinical$Sample)
length(full.gene.clin)
```

The first intersection that is shown is the intersection between the gene expression data with no missing values and the clinical data, i.e. this intersection now contains all unique patient identifiers that exist in the gene data and that have recorded the response. Notice that this the data that will be used in the first part of the task. 

```{r}
int.gene.prot <- intersect(gene.exp$Sample, prot.ab$Sample) 
length(int.gene.prot)
```

The second intersection that is shown is the intersection between the gene expression data which still contains missing values and the protein abundance data. This will not be used in the analysis, but is given because it might be interesting to keep in mind. Thus we can see that `r length(int.gene.prot)` of the patients' sample codes exist in both the protein abundance and the gene expression datasets, before removing the rows with missing values from the gene expression data. 

```{r}
int.gene.full.prot <- intersect(gene.exp2$Sample, prot.ab$Sample)
length(int.gene.full.prot)
```

The intersection above gives the unique patients that have no missing gene expression data and have recorded protein abundance data. This is used in order to define the next intersection. 

```{r}
full.gene.prot.clin <- intersect(int.gene.full.prot, clinical$Sample) 
length(full.gene.prot.clin)
```

The last intersection gives the intersection between the third list of patients (`int.gene.full.prot`) and the patients in the `clinical` data set. Thus, this contains the unique list of patients that have all necessary data in all three data sets. Notice that these patients will be used to define the complete data set, which will be used for the concatenated model later in the analysis (questions 7-10 in the task description). 

As noted earlier, we will now only use the intersection `full.gene.clin` (to answer questions 2-6 in the task description). Even though we know that all these patients have a recorded breast invasive carcinoma (BRCA) estrogen receptor status, we need to check that the values they have recorded are either `Positive` or `Negative`, keeping in mind the values we saw that the `clinical` dataset contains. Next, we remove all the individuals that don't have this information. 

```{r}
chosen.data <- full.gene.clin

xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate", "Not Performed" or NA.
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
```

## Gene Expression Data

Again, it is stressed that we now only use the **gene expression data**, i.e. the first mentioned set above (`full.gene.clin`). After the previous pre-process, this data set now contains the patients that have the complete gene expression data, as well as a well-defined BRCA receptor status.

### Select the 25% of genes with the most variability.

The 25% percent of genes with the most variability are chosen. Information about the genes chosen are stored and reused later in the selection of the genes for the complete data set in section 4 and onwards, to make sure that the same set of genes (features) that the SAE was trained with are the ones selected in the complete data (set as well. 

```{r}
percentage <- round(dim(mgene[,-c(1,2)])[[2]]*0.25) # Find how many variables correspond to 25%. 
variances <- apply(X=mgene[,-c(1,2)], MARGIN=2, FUN=var) # Find empirical variance in each of the variables (genes).
sorted <- sort(variances, decreasing=TRUE, index.return=TRUE)$ix[1:percentage] # Sort from highest to lowest variance and select the top 25% indices. 
mgene.lvar <- mgene[, c(1,2,sorted)] # Select the 25% largest variance variables using the indices found above. 
```

The selected `r percentage` genes are used to implement a stacked autoencoder (SAE) with three stacked layers of 1000, 100 and 50 nodes. 

## Final Training/Test Split

```{r}
set.seed(params$seed)
training.fraction <- 0.70 # 70 % of data will be used for training. 
training <- sample(1:nrow(mgene.lvar),nrow(mgene.lvar)*training.fraction) 

xtrain <- mgene.lvar[training,-c(1,2)]
xtest <- mgene.lvar[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 
xtrain <- scale(data.matrix(xtrain)) 
xtest <- scale(data.matrix(xtest))

# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0

# The data is saved to a file, so that it can be loaded directly into tfruns() files. 
data.train <- data.frame(ylabels, xtrain)
data.test <- data.frame(ytestlabels, xtest)
write.csv(data.train, "train_for5.csv")
write.csv(data.test, "test_for5.csv")
```

# Implementation of SAE

In this section a stacked autoencoder (SAE) will be implemented. It will consist of three stacked layers of 1000, 100 and 50 nodes. In each case, some qualitative evidence of the quality of coding obtained will be given, in the form of correlation plots between input and output. 

## First Layer (1000 nodes)
```{r}
# Develop the encoder. 
input_enc1 <- layer_input(shape = percentage)
output_enc1 <- input_enc1 %>% 
  layer_dense(units=1000,activation="relu") 
encoder1 <- keras_model(input_enc1, output_enc1)
summary(encoder1)

# Develop the decoder. 
input_dec1 <- layer_input(shape = 1000)
output_dec1 <- input_dec1 %>% 
  layer_dense(units = percentage, activation="linear")
decoder1 <- keras_model(input_dec1, output_dec1)
summary(decoder1)

# Develop the first AE.
aen_input1 <- layer_input(shape = percentage)
aen_output1 <- aen_input1 %>% 
  encoder1() %>% 
  decoder1()
sae1 <- keras_model(aen_input1, aen_output1)
summary(sae1)
```

We compile the model and fit it to the training data. To decide the final number of epochs each 'val-loss'-value is regarded after each epoch. If the value has not decreased in a certain number of epochs the training will stop. This is to reduce the risk of overfitting the network, that is that the network may otherwise learn patterns that are too specific for the training data while the performance on the actual validation data begins to decrease. 

```{r comp}
sae1 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r fit}
callbacks_parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 12,
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

sae1 %>% fit(
  x = xtrain,
  y = xtrain,
  epochs = 40,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data. 

```{r}
encoded_expression1 <- encoder1 %>% predict(xtrain) 
decoded_expression1 <- decoder1 %>% predict(encoded_expression1)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae1,xtrain)
```

Some (weak) evidence of the quality of the coding obtained follows. We plot the correlation between the predictions from the autoencoder and the correct data, both on the train and test sets. The results are shown below. 

```{r}
vcor <- diag(cor(x.hat,xtrain))
hist(vcor, main = "Correlation on Training Data")
```

The histogram above shows that the correlation between the training data and the predictions from the first autoencoder are relatively high. Worth mentioning is that if we had not implemented the training stop when no progression of the validation data was registered, the correlation on the training set would be significantly higher, as the network would continue to fit the parameters to the training data. As stated, this would probably have resulted in worse values for the initial test set that we set aside before the training started. We do the same check on the test set. 

```{r}
x.hat <- predict(sae1,xtest)
vcor <- diag(cor(x.hat,xtest))
hist(vcor, main = "Correlation on Testing Data")
```

As expected, the correlation is lower on the test data, but there still is some correlation. 

## Second Layer (100 nodes)
```{r}
# Develop the encoder. 
input_enc2 <- layer_input(shape = 1000)
output_enc2 <- input_enc2 %>% 
  layer_dense(units=100,activation="relu") 
encoder2 <- keras_model(input_enc2, output_enc2)
summary(encoder2)

# Develop the decoder. 
input_dec2 <- layer_input(shape = 100)
output_dec2 <- input_dec2 %>% 
  layer_dense(units = 1000, activation="linear")
decoder2 <- keras_model(input_dec2, output_dec2)
summary(decoder2)

# Develop the second AE.
aen_input2 <- layer_input(shape = 1000)
aen_output2 <- aen_input2 %>% 
  encoder2() %>% 
  decoder2()
sae2 <- keras_model(aen_input2, aen_output2)
summary(sae2)
```

We compile the model and fit it to the training data. 

```{r}
sae2 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
callbacks_parameters <- callback_early_stopping(
  monitor = "val_loss",
  patience = 8,
  verbose = 1,
  mode = "min",
  restore_best_weights = FALSE
)

sae2 %>% fit(
  x = encoded_expression1,
  y = encoded_expression1,
  epochs = 100,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder. Also here we use the automatic training stop when the performance on the validation set has stopped improving.

```{r}
encoded_expression2 <- encoder2 %>% predict(encoded_expression1) 
decoded_expression2 <- decoder2 %>% predict(encoded_expression2)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae2,encoded_expression1)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression1))
hist(vcor, main = "Correlation with Input Data (Encoded from sae1)")
```

The histogram above shows that there is correlation between the input and the predictions from the second autoencoder. Note that we do not have a test set in this case, since the dimension of the output data from sae2 is 1000, which is less than the amount of features in the test data. 

## Third Layer (50 nodes)
```{r}
# Develop the encoder. 
input_enc3 <- layer_input(shape = 100)
output_enc3 <- input_enc3 %>% 
  layer_dense(units=50,activation="relu") 
encoder3 <- keras_model(input_enc3, output_enc3)
summary(encoder3)

# Develop the decoder. 
input_dec3 <- layer_input(shape = 50)
output_dec3 <- input_dec3 %>% 
  layer_dense(units = 100, activation="linear")
decoder3 <- keras_model(input_dec3, output_dec3)
summary(decoder3)

# Develop the third AE.
aen_input3 <- layer_input(shape = 100)
aen_output3 <- aen_input3 %>% 
  encoder3() %>% 
  decoder3()
sae3 <- keras_model(aen_input3, aen_output3)
summary(sae3)
```

We compile the model and fit it to the training data. 

```{r}
sae3 %>% compile(
  optimizer = "rmsprop",
  loss = "mse"
)
```

```{r}
sae3 %>% fit(
  x = encoded_expression2,
  y = encoded_expression2,
  epochs = 180,
  batch_size = 64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

We make predictions on the training data, which in this case is the training data reduced in dimension from the first autoencoder.  

```{r}
encoded_expression3 <- encoder3 %>% predict(encoded_expression2) 
decoded_expression3 <- decoder3 %>% predict(encoded_expression3)

# This method gives the same predictions as the two lines above. 
# i.e. the values in decoded_expression above are the same as the values in x.hat below. 
x.hat <- predict(sae3,encoded_expression2)
```

Some (weak) evidence of the quality of the coding obtained follows. Now we plot the correlation between the predictions from the autoencoder and the input data, which in this case is the encoded data from the first autoencoder (the latent variables/space). 

```{r}
vcor <- diag(cor(x.hat,encoded_expression2))
hist(vcor, main = "Correlation with Input Data (Encoded from sae2)")
```

The histogram above shows that there is some correlation between the input and the predictions from the second autoencoder. Notice that the correlation is less than earlier though. Also note that we do not have a test set in this case, since the dimension of the output data from sae3 is 100, which is less than the amount of features in the test data. 

## Final Model (SAE)

The final stacked autoencoder (SAE) is constructed below. All the encoders previously trained are stacked together. 

```{r}
sae_input <- layer_input(shape = percentage, name = "gene.mod")
sae_output <- sae_input %>% 
  encoder1() %>% 
  encoder2() %>%
  encoder3()
   
sae <- keras_model(sae_input, sae_output)
summary(sae)

# Code below is used for loading model (and model checking) in separate file for tfruns().
yhat <- predict(sae, xtest)
write.csv(yhat, file = "predictions.csv")
save_model_weights_hdf5(sae, "sae.hdf5")
```

# SAE as Pre-Training Model for Prediction of Estrogen Receptor State

The SAE is used as a pre-training model for prediction of the estrogen receptor state. The DNN has 10 nodes in the first layer, followed by one output node. The weights are frozen for the first 3 functional layers, which means that only the weights from the third autoencoder to the first fully connected layer and from the first layer in the DNN to the output layer (in total 521 weights) are to be fine-tuned to obtain the final classifier. 

```{r}
sae_output2 <- sae_output %>%
  layer_dense(10,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)
```

We compile and fit the final classifier. 

```{r}
sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)
```

  
```{r}
sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 120,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)
```

## Performance Metrics

The model is evaluated on the test set below.  

```{r}
sae %>%
  evaluate(xtest, ytestlabels)
```

Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below. 

```{r}
yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))
```

The ROC curve is shown below. 

```{r}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

As is seen above, the AUC is `r round(roc_sae_test$auc, 2)`.

For an AUC curve, a score of 1 would signify a perfect predictor whilst a score of 0.5 would signify that the predictor is just as good as a random generator (i.e. equivalent to flipping a coin). A score of `r round(roc_sae_test$auc, 2)` is therefore considered acceptable as it should be able to distinguish the various cases to some extent, however there is big room for improvement.

# Use `tfruns` to Explore Configurations of First Fully Connected Layer

In this section, `tfruns` is used to explore what amount of nodes in the first layer of the DNN gives the best performance. We are asked to search among three different numbers; 5, 10 and 20. The code used with `tfruns` is given in separate R files. The exploration leads to the conclusion that the configuration with 20 nodes gives the best results. 

Thus, the final model is

```{r}
sae_output2 <- sae_output %>%
  layer_dense(20,activation = "relu") %>% # Couple with fully connected layers (DNN).
  layer_dense(1,activation = "sigmoid")

sae <- keras_model(sae_input, sae_output2)
summary(sae)

freeze_weights(sae,from=1,to=4) # Freeze the weights (pre-training using the SAE).
summary(sae)

sae %>% compile(
  optimizer = "rmsprop",
  loss = 'binary_crossentropy',
  metric = "acc"
)

sae %>% fit(
  x=xtrain,
  y=ylabels,
  epochs = 80,
  batch_size=64,
  validation_split = 0.2,
  callbacks = callbacks_parameters
)

sae %>%
  evaluate(xtest, ytestlabels)

yhat <- predict(sae,xtest)
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(ytestlabels))

roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```
From the ROC curve we see that the AUC-score now is closer to 1, indicating that this is a better model.

## Complete Data 

The **complete data** (gene expression and protein abundance) is split into train and test sets. 

As explained in section 1, the same set of gene features is used in this dataset, in order to secure that the input now consists of the same genes that the network was trained on in previous sections. 

```{r}
chosen.data <- full.gene.prot.clin # Complete dataset, as defined in part 1. 
xclin <- clinical[,c(1,9)]
colnames(xclin) <- c("Sample", "BRCA")
xclin <- xclin[clinical$Sample %in% chosen.data, ] 
xprot <- prot.ab[prot.ab$Sample%in%chosen.data,]
xgene <- gene.exp2[gene.exp2$Sample %in% chosen.data, ]

sel1 <- which(xclin$BRCA != "Positive")
sel2 <- which(xclin$BRCA != "Negative")
sel <- intersect(sel1,sel2) # Find values of BRCA that are not negative or positive. 
# In this case these values are either "Indeterminate", "Not Performed" or NA. 
xclin <- xclin[-sel,] # Remove the rows with non-valid data for BRCA. 
xclin <- xclin[-which(is.na(xclin$BRCA)),] # Also remove rows with missing data for BRCA. 

# Join the (cleaned) clinical data and the gene expression data on "Sample".
mgene <- merge(xclin, xgene, by.x = "Sample", by.y = "Sample")
mtot <- merge(mgene, xprot, by.x = "Sample", by.y = "Sample")
#mtot <- xclin %>% left_join(xgene) %>% left_join(xprot, by = "Sample") # This gives the same result. 

mtot.patients <- mtot[,1]

set.seed(params$seed)
training.fraction <- 0.70 # 70 % of data will be used for training. 
training <- sample(1:nrow(mtot),nrow(mtot)*training.fraction) 

#xtrain <- mtot[training,-c(1,2)]
#xtest <- mtot[-training,-c(1,2)]

# Scaling for better numerical stability. 
# This is a standard "subtract mean and divide by standard deviation" scaling. 

#xtrain <- scale(data.matrix(xtrain)) 
#xtest <- scale(data.matrix(xtest))

# Pick out the same features as in the first model (as noted in section 1.1.1)
#mtot.selected.genes <- xgene[, c(1,2,sorted)] # Select the 25% largest variance variables using the indices found in section 1.1.1.
# Pick out labels for train and test set. 
#ytrain <- mtot.lvar[training,2]
#ytest <- mtot.lvar[-training,2]

# Change labels to numerical values in train and test set. 
#ylabels <- c()
#ylabels[ytrain=="Positive"] <- 1
#ylabels[ytrain=="Negative"] <- 0

#ytestlabels <- c()
#ytestlabels[ytest=="Positive"] <- 1
#ytestlabels[ytest=="Negative"] <- 0
```

```{r}
xprot2 <- xprot %>% dplyr::filter(Sample %in% mtot.patients)
xprot.train <- xprot2[training,-1]
xprot.test <- xprot2[-training,-1]

xgene2 <- xgene %>% dplyr::filter(Sample %in% mtot.patients)

# Pick out the same features as in the first model (as noted in section 1.1.1)
xgene2 <- xgene2[, sorted] # Select the 25% largest variance variables using the indices found in section 1.1.1.
#xgene.train <- xgene2[training,-1]
#xgene.test <- xgene2[-training,-1]
# Ändrar dessa 
xgene.train <- xgene2[training,]
xgene.test <- xgene2[-training,]

# OLD:
# Denne er åpenbart feil!!! Fiks denne nå, så er alt i orden!!
# ytrain_bin <- to_categorical(as.array(ylabels), 2)

# NEW try below:
# Pick out labels for train and test set. 
ytrain <- mgene.lvar[training,2]
ytest <- mgene.lvar[-training,2]

# Change labels to numerical values in train and test set. 
ylabels <- c()
ylabels[ytrain=="Positive"] <- 1
ylabels[ytrain=="Negative"] <- 0

ytestlabels <- c()
ytestlabels[ytest=="Positive"] <- 1
ytestlabels[ytest=="Negative"] <- 0

yLabelsStored <- ylabels

```

## Importing the SAE from the class example.

The SAE for the abundance of proteins (from class examples) is added in the code block below. Then, this model is concatenated with the former model, that was trained on the gene expression data. 


```{r}
# Model given in class. 
source("aes_practice_3.R") # source: Parses the code and evaluates it in this environment. 
```


# Concatenated Model

In this section we concatenate the two SAEs to fit, on the trainset, a DNN that integrates both data sources to predict estrogen receptor status. The DNN has a dense layer, with 20 nodes, which where the best amount of nodes found with `tfruns` earlier, in addition to the output layer. 

The two models are concatenated below. 

```{r}



sae_output2 <- sae_input %>% 
  encoder1() %>% 
  encoder2() %>%
  encoder3()
  layer_dense(units=20, activation = "relu")

sae_protab_output <- sae_protab_input %>%
  prot_encoder1() %>% 
  prot_encoder2()  %>%
  prot_encoder3() %>%
  layer_dense(units=20, activation = "relu")


concatenated<-layer_concatenate(list(sae_output2,sae_protab_output))
#concatenated<-layer_concatenate(list(sae_output,sae_protab_output))
#model_output<-concatenated %>% 
#  layer_dense(20,"relu") %>% 
#  layer_dense(units=1,activation="softmax")
# Ändrar till sigmoid istället
model_output<-concatenated %>%
  #layer_dense(units=2,activation="softmax") %>%
  #layer_dense(20,"relu") %>%
  #layer_dense(units=20,activation="relu") #%>%
  #layer_dense(units=1,activation="sigmoid")
  layer_dense(units=2,activation="softmax")

model<-keras_model(list(sae_input,sae_protab_input), model_output)
summary(model)
```


```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "acc"
)
#optimizer = "opt",
# loss = "categorical_crossentropy",

# OLD:
# training 
#Den her funker ikke. Jeg tror det er fordi xgene_train har 17000 ish kolonner. Den burde egentlig ha typ 4454 eller noe liknende. Må se litt på datarensing. (kan også hende jeg har gjort ting helt feil i tillegg)
#model %>% fit(
#  list(gene.mod=xgene_train, prot.mod=xprot_train), as.array(yLabelsStored),
#  epochs = 75, batch_size = 64, validation_split = 0.2
#)


model %>% fit(
  x=list(as.matrix(xgene.train),as.matrix(xprot.train)),
  y=as.matrix(to_categorical(yLabelsStored,2)),
  epochs = 75,
  batch_size=64,
  validation_split = 0.2#,
  #callbacks = callbacks_parameters
)

#CHECK: do we need callbacks here?

#model %>% fit(
#  x=list(xgene.train,xprot.train),
#  y=as.matrix(yLabelsStored),
#  epochs = 80,
#  batch_size=64,
#  validation_split = 0.2,
#  callbacks = callbacks_parameters
#)

```

```{r, eval=F}

#This part is not working.
# training_on_batch

#for(i in 1:100){
  
  #sel1<-sample(1:40,16)
#  sel1<-sample(1:123,32,replace=T)
#  batchdata1<-xgene.train[sel1,,1]
#  batchdata1<-array_reshape(batchdata1,dim=c(32,percentage,1))
#  batchdata2<-xprot.train[sel1,,1]
#  batchdata2<-array_reshape(batchdata2,dim=c(32,142,1))
#  combined<-list(xgene.train,xprot.train)
#  batch.y<-yLabelsStored[c(sel1),]
  
  
#  dloss<-model %>% train_on_batch(combined, batch.y)
#  print(dloss)
  
  
#}

```


## Performance Metrics

The model is evaluated on the test set below.  

```{r, eval = F}
#This is not working due to ytestlabels having different dimensions from xgenetest and xprotest
model %>%
  evaluate(x=list(as.matrix(xgene.test),as.matrix(xprot.test)),
  y=as.matrix(to_categorical(ytestlabels,2)),)
```

<!-- Predictions on the test set are calculated. The classifier is built on the assumption that predictions of probability smaller than $0.5$ are negative receptor states, while probabilities larger than $0.5$ are positive receptor states. The confusion matrix of the predictions is shown below.  -->

```{r, eval = F}
yhat <- predict(model,list(as.matrix(xgene.test),as.matrix(xprot.test)))
yhatclass<-as.factor(ifelse(yhat<0.5,0,1))
confusionMatrix(yhatclass,as.factor(to_categorical(ytestlabels,2)))
```

The ROC curve is shown below. 

```{r, eval = F}
roc_sae_test <- roc(response = ytestlabels, predictor = as.numeric(yhat))
plot(roc_sae_test, col = "blue", print.auc=TRUE)
legend("bottomright", legend = c("sae"), lty = c(1), col = c("blue"))
```

# Discussion

There seems to be some different angles to address the first step of processing the datasets. The removal of the patients samples with missing values in the gene expression dataset seems to be an important step to secure that all used patients has complete data to pass as input to the network. The question then arise if we only should include the patients samples that both exist in the gene expression dataset and the protein abundance dataset when implementing the stacked autoencoder, to then be able to use the same set for the concatenate version in the later stage. Since the first part asked specific to implement a SAE with the gene expression data we choose the alternative that was to
include all complete patients in the gene expression dataset, with no consideration of they existed in the protein abundance dataset. Or motivation was that this should work as least as good as the alternative for the SAE, this method gave more patients left in the actual used training set. 

One thing to remember when choosing the above described way is that we should make sure that the same genes that were selected for the training of the first SAE are also the selected in the preparation of the joint dataset for the concatenate SAE. Otherwise we risk training the first SAE on one set of genes from each patient and then using it on a potential other set of genes. 

In the training of the SAE one difficulty is to avoid both underfitting and overfitting on the training data. With the aim to run each training for a suitable amount of epochs each training were monitored so that it could not keep training the network if the performance on the validation set did not increase during a fixed number of epochs. The intention is that this will lead to a network with better performance on a test set. 

WE CURRENTLY CANNOT MAKE ANY DISCUSSION OR COMPARISON ABOUT THE PERFORMANCE CONCATENATED MODEL. 
